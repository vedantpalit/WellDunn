{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14Xf3JB89PXZmkoV574WkyV9Zo64qk_4E","timestamp":1690351807126},{"file_id":"1Pyu70NsBE7ikh0UnVwpO-7TwFErveW-z","timestamp":1690258027891}],"gpuType":"T4","collapsed_sections":["IzxARdRKPVY_","Wf28VvAsRkaF","0CYNNM4XSmHP"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install sentencepiece\n","!pip install transformers\n","!pip install tensorflow_addons"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fjncg9GrtQE2","executionInfo":{"status":"ok","timestamp":1690473099207,"user_tz":-330,"elapsed":24275,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}},"outputId":"de630876-a8a4-4b44-fb20-8f04355945f3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow_addons\n","Successfully installed tensorflow_addons-0.21.0 typeguard-2.13.3\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMobvUxduUL0","executionInfo":{"status":"ok","timestamp":1690473156791,"user_tz":-330,"elapsed":50303,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}},"outputId":"080522e8-c8fe-4d19-9d93-c430cf8014bb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer\n","from transformers import XLNetModel, XLNetTokenizer\n","import torch"],"metadata":{"id":"FTJKH48utvSW","executionInfo":{"status":"ok","timestamp":1690473164687,"user_tz":-330,"elapsed":5363,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","data=pd.read_csv('/content/drive/MyDrive/Wellness_Dataset_Paper/MultiLabel_WD - Sheet1 (1) (1).csv')\n"],"metadata":{"id":"FVSDIkDbYmlV","executionInfo":{"status":"ok","timestamp":1690473973928,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["#6 Dimensions"],"metadata":{"id":"IzxARdRKPVY_"}},{"cell_type":"code","execution_count":20,"metadata":{"id":"EGVrP2EOBpCC","executionInfo":{"status":"ok","timestamp":1690473973928,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["data=data.astype({'Spiritual':'float', 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float',\n","       'Emotional':'float'})"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"LV3_VLoTBqon","executionInfo":{"status":"ok","timestamp":1690473973928,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["dimensions_list = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational','Emotional']\n","dimension=6"]},{"cell_type":"markdown","source":["#5 Dimensions"],"metadata":{"id":"Wf28VvAsRkaF"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"BS4S2KGLCysI","executionInfo":{"status":"ok","timestamp":1690473975963,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["data=data.astype({'Spiritual':'int32', 'Physical':'int32', 'Intellectual':'int32', 'Social':'int32', 'Vocational':'int32',\n","       'Emotional':'int32'})"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"CQwX98YPC0pq","executionInfo":{"status":"ok","timestamp":1690473977124,"user_tz":-330,"elapsed":1,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["data ['Spiritual or Emotional'] = data.Spiritual | data.Emotional"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"_vXb_oGNC2Ww","executionInfo":{"status":"ok","timestamp":1690473978422,"user_tz":-330,"elapsed":1,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["data = data.drop(['Spiritual', 'Emotional'], axis=1)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"o7bkXLqBC4PV","executionInfo":{"status":"ok","timestamp":1690473978942,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["data=data.astype({ 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float',\n","       'Spiritual or Emotional':'float'})"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"MdDESklpC7AJ","executionInfo":{"status":"ok","timestamp":1690473979647,"user_tz":-330,"elapsed":1,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"outputs":[],"source":["dimensions_list = ['Physical', 'Intellectual', 'Social', 'Vocational','Spiritual or Emotional']\n","dimension=5"]},{"cell_type":"markdown","source":["#4 Dimensions"],"metadata":{"id":"0CYNNM4XSmHP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdOOfZNaDXVb"},"outputs":[],"source":["data=data.astype({ 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float',\n","       'Spiritual or Emotional':'float'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82j193L-Dd3J"},"outputs":[],"source":["data=data.astype({'Spiritual or Emotional':'int32', 'Physical':'int32', 'Intellectual':'int32', 'Social':'int32', 'Vocational':'int32',\n","       })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jRhTaH8DfT2"},"outputs":[],"source":["data ['Spiritual or Emotional or Physical'] = data ['Spiritual or Emotional'] | data['Physical']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WHbBtl2Dg6c"},"outputs":[],"source":["data = data.drop(['Physical'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFxebINXDiLu"},"outputs":[],"source":["data=data.drop(['Spiritual or Emotional'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tl52AAkIDjm1"},"outputs":[],"source":["data=data.astype({  'Intellectual':'float', 'Social':'float', 'Vocational':'float',\n","       'Spiritual or Emotional or Physical':'float'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WM9bv7lcDlYp"},"outputs":[],"source":["dimensions_list = ['Intellectual', 'Social', 'Vocational','Spiritual or Emotional or Physical']\n","dimension=4"]},{"cell_type":"markdown","source":["#Main"],"metadata":{"id":"wUbRZ0_aVZSW"}},{"cell_type":"code","source":["MAX_LEN = 64\n","TRAIN_BATCH_SIZE = 2\n","VALID_BATCH_SIZE = 2\n","EPOCHS = 5\n","LEARNING_RATE= 1e-05\n","\n","#dimension = 6\n","#dimensions_list = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational','Emotional']\n","threshold = 1 #\n","Classifiers = [\"nghuyong/ernie-2.0-en\", \"bert-base-uncased\",\"roberta-base\" ,\"emilyalsentzer/Bio_ClinicalBERT\", \"xlnet-base-cased\",'nlptown/bert-base-multilingual-uncased-sentiment', \"mental/mental-bert-base-uncased\"]\n","Classifiers_Abs = [\"ERNIE\", \"BERT\", \"RoBERTa\", \"ClinicalBERT\", \"XLNET\", \"PsychBERT\", \"Mental-BERT\"]\n","classifier_index=6#set as needed\n","TheClassifier = Classifiers[classifier_index]\n","TheClassifier_Abstract = Classifiers_Abs[classifier_index]\n","\n","rand_states = [345, 546,200]\n","ran_index = 1\n","rand_state = rand_states[ran_index]\n","\n","target_List = dimensions_list"],"metadata":{"id":"berITLWfZIK-","executionInfo":{"status":"ok","timestamp":1690473985857,"user_tz":-330,"elapsed":473,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["import torch\n","device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self, df, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = df['text']\n","        self.targets = self.df[target_List].values\n","        self.max_len = max_len\n","\n","  def __len__(self):\n","        return len(self.title)\n","\n","  def __getitem__(self, index):\n","        # print(index,\":\",self.title[index])\n","        # print(self.title[index])\n","        title = self.title[index]\n","        title = \" \".join(title.split())\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index])\n","        }"],"metadata":{"id":"nLdQ3zL0Zv7v","executionInfo":{"status":"ok","timestamp":1690473989279,"user_tz":-330,"elapsed":827,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["if classifier_index==0:\n","  tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-2.0-en')\n","  class ERNIEClass(torch.nn.Module):\n","    def __init__(self):\n","        super(ERNIEClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-2.0-en')\n","        self.ernie_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-en', output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, token_type_ids):\n","        output_dict = self.ernie_model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=token_type_ids,\n","            output_hidden_states=True,\n","            return_dict=True\n","        )\n","        last_hidden_state = output_dict.last_hidden_state\n","        attention_weights = output_dict.attentions\n","        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n","        output = self.linear(output_dropout)\n","        return output, attention_weights[-1]\n","\n","  model = ERNIEClass()\n","  model.to(device)\n","\n","if classifier_index==1:\n","  tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n","  class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = AutoModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","\n","\n","  model = BERTClass()\n","  model.to(device)\n","\n","if classifier_index==2:\n","  tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n","  class roBERTaClass(torch.nn.Module):\n","    def __init__(self):\n","        super(roBERTaClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","        self.model = AutoModel.from_pretrained('roberta-base',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","\n","\n","\n","\n","  model = roBERTaClass()\n","  model.to(device)\n","\n","if classifier_index==3:\n","  tokenizer=AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","  class ClinicalBIGBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(ClinicalBIGBERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","        self.model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","  model = ClinicalBIGBERTClass()\n","  model.to(device)"],"metadata":{"id":"P-fW-PUnPtFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if classifier_index==4:\n","  tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n","  class XLNETClass(torch.nn.Module):\n","    def __init__(self):\n","        super(XLNETClass, self).__init__()\n","        self.xlnet_model = XLNetModel.from_pretrained(\"xlnet-base-cased\", output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, token_type_ids):\n","        output_dict = self.xlnet_model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=token_type_ids,\n","            output_hidden_states=True,\n","            return_dict=True\n","        )\n","        last_hidden_state = output_dict.last_hidden_state\n","        attention_weights = output_dict.attentions\n","        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n","        output = self.linear(output_dropout)\n","        return output, attention_weights[-1]\n","\n","  model = XLNETClass()\n","  model.to(device)\n","elif classifier_index==5:\n","  tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n","  class PsychBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(PsychBERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n","        self.model = AutoModel.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768,dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.pooler_output)\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","  model = PsychBERTClass()\n","  model.to(device)\n","\n","elif classifier_index==6:\n","  !huggingface-cli login\n","  tokenizer=AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased',use_auth_token=True)\n","  class MentalBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(MentalBERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased')\n","        self.model = AutoModel.from_pretrained('mental/mental-bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","  model = MentalBERTClass()\n","  model.to(device)"],"metadata":{"id":"MYKdDh35Zy-j","executionInfo":{"status":"ok","timestamp":1690474007306,"user_tz":-330,"elapsed":13318,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb18d00f-e2f5-4510-fa1b-89bb1d556581"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","    \n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["test_size = 0.2\n","val_df = data.sample(frac=test_size, random_state=rand_state).reset_index (drop=True)\n","train_df = data.drop (val_df.index).reset_index (drop=True)\n","\n","\n","\n","train_dataset=CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset=CustomDataset(val_df, tokenizer,MAX_LEN)\n","\n","train_data_loader = torch.utils.data.DataLoader (\n","train_dataset,\n","shuffle=True,\n","batch_size=TRAIN_BATCH_SIZE,\n","num_workers=0\n",")\n","val_data_loader = torch.utils.data.DataLoader (\n","valid_dataset,\n","shuffle=True,\n","batch_size=VALID_BATCH_SIZE,\n","num_workers=0\n",")"],"metadata":{"id":"_kXFUYmjZ2aT","executionInfo":{"status":"ok","timestamp":1690474012187,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into\n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss\n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n","\n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    print(\"checkpoint_path:\",checkpoint_path)\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        # copy that checkpoint file to best path given, best_model_path\n","        shutil.copyfile(f_path, best_fpath)\n"],"metadata":{"id":"_nWQg1s2Z48j","executionInfo":{"status":"ok","timestamp":1690474014328,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def loss_fn(m_outputs, targets):\n","        reward = dimension\n","\n","        tensor_temp = torch.zeros(32,dtype=torch.float)\n","        tensor_temp.to(device)\n","        outputs = torch.nn.functional.softmax(m_outputs, dim=1,dtype=torch.float)\n","\n","        outputs, reservation = outputs[:, :-1], outputs[:, -1]\n","\n","        # gain = torch.gather(outputs, dim=1, index=targets).squeeze()\n","        # print(\"targets:\",targets)\n","        # print(\"outputs:\", outputs)\n","        # raise KeyboardInterrupt\n","        # return targets, outputs\n","        gain = torch.einsum(\"ij, ij -> i\", targets.to(torch.float), outputs)\n","\n","        # doubling_rate = (gain.max() + reservation / reward).log()\n","        doubling_rate = -torch.log(gain + reservation/reward)\n","        return  doubling_rate.mean(), reservation"],"metadata":{"id":"fMuwnovWZ7Zl","executionInfo":{"status":"ok","timestamp":1690474017544,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n","\n","\n","\n","val_targets=[]\n","val_outputs=[]"],"metadata":{"id":"DtGKWM2BZ9GK","executionInfo":{"status":"ok","timestamp":1690474020269,"user_tz":-330,"elapsed":755,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["\n","\n","def train_model(n_epochs, training_loader, validation_loader, model,\n","                optimizer, checkpoint_path, best_model_path):\n","\n","  # initialize tracker for minimum validation loss\n","  valid_loss_min = np.Inf\n","\n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0\n","    valid_loss = 0\n","\n","    model.train()\n","\n","    for batch_idx, data in enumerate(training_loader):\n","        # print(data['input_ids'])\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        # print(ids)\n","        # raise KeyboardInterrupt\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","\n","        outputs, _ = model(ids, mask, token_type_ids)\n","\n","        optimizer.zero_grad()\n","        loss, reservationn = loss_fn(outputs, targets.type(torch.int64))\n","\n","        # print(outputs)\n","        # loss2 = loss_fn2(outputs, targets)\n","\n","        # print(\"loss gambler: \",loss)\n","        # print(\"reservation: \", reservationn)\n","\n","        # print(\"loss2 CE: \", loss2)\n","\n","        # raise KeyboardInterrupt\n","        # tar, outp = loss_fn(outputs, targets.type(torch.int64))\n","        # return tar, outp\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # print('before loss data in training', loss.item(), train_loss)\n","        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","        #print('after loss data in training', loss.item(), train_loss)\n","\n","    # print('############# Epoch {}: Training End     #############'.format(epoch))\n","\n","    # print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################\n","    # validate the model #\n","    ######################\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(validation_loader, 0):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs, _ = model(ids, mask, token_type_ids)\n","\n","            loss, _ = loss_fn(outputs, targets.type(torch.int64))\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","            val_targets.extend(targets.cpu().detach().numpy().tolist())\n","            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","      # print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      # calculate average losses\n","      #print('before cal avg train loss', train_loss)\n","      train_loss = train_loss/len(training_loader)\n","      valid_loss = valid_loss/len(validation_loader)\n","      # print training/validation statistics\n","      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n","            epoch,\n","            train_loss,\n","            valid_loss\n","            ))\n","\n","      # create checkpoint variable and add important data\n","      checkpoint = {\n","            'epoch': epoch + 1,\n","            'valid_loss_min': valid_loss,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","      }\n","        # save checkpoint\n","      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","\n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n","        # save checkpoint as best model\n","        # save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","        valid_loss_min = valid_loss\n","\n","    # print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","\n","  return model"],"metadata":{"id":"NAVVGDhDaBcz","executionInfo":{"status":"ok","timestamp":1690474020758,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["import shutil, sys\n","\n","trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, \"ckpt_path/themodel3.pt\", \"thebestone3.pt\")"],"metadata":{"id":"MeGYXqfcaEFj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"21b297ff-72a1-443d-ef80-43bae20585da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 \tAvgerage Training Loss: 0.000451 \tAverage Validation Loss: 0.001477\n","Validation loss decreased (inf --> 0.001477).  Saving model ...\n"]}]},{"cell_type":"code","source":["last_layer_attentions = []\n","for batch_idx, data in enumerate(train_data_loader):\n","    ids = data['input_ids'].to(device, dtype=torch.long)\n","    mask = data['attention_mask'].to(device, dtype=torch.long)\n","    token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","    outputs, output_with_attention = model(ids, mask, token_type_ids)\n","\n","    if classifier_index == 4 or classifier_index==0:\n","        attentions = output_with_attention  # For XLNET\n","    else:\n","        attentions = output_with_attention.attentions[0]\n","\n","    for sample in attentions:\n","        last_layer_attentions.append((sample[11]).detach().cpu())  # Detach and move to CPU\n","\n","    # Clear GPU memory\n","    del ids, mask, token_type_ids, outputs, output_with_attention\n","    torch.cuda.empty_cache()"],"metadata":{"id":"WEpokgmfaJb_","executionInfo":{"status":"ok","timestamp":1690473941478,"user_tz":-330,"elapsed":32544,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S64AP8ndqfsN","executionInfo":{"status":"ok","timestamp":1690473948973,"user_tz":-330,"elapsed":5807,"user":{"displayName":"Vedant Palit","userId":"05232341091904350034"}},"outputId":"fa565182-8cb0-4936-fdb2-66e26af5ae67"},"outputs":[{"output_type":"stream","name":"stdout","text":["***********************************\n","Model: Mental-BERT\n","Dimensions: 6\n","Epochs: 5\n","Batch_size: 2\n","Max_len: 64\n","Learning Rate: 1e-05\n","Rand_state: 345\n","SVD_ranking : 45\n"]}],"source":["from numpy.linalg import svd\n","from numpy.linalg import matrix_rank\n","\n","d=[item.detach().numpy() for item in last_layer_attentions]\n","U, S, VT = svd(d)\n","print('***********************************')\n","print(\"Model:\", TheClassifier_Abstract)\n","print(\"Dimensions:\",dimension)\n","print(\"Epochs:\", EPOCHS)\n","print(\"Batch_size:\",TRAIN_BATCH_SIZE)\n","print(\"Max_len:\", MAX_LEN)\n","print(\"Learning Rate:\", LEARNING_RATE)\n","print(\"Rand_state:\", rand_state)\n","print(\"SVD_ranking :\", matrix_rank(S))\n","\n","\n","# final_list=[]\n","# for i in val_df['Text']:\n","#   example = i\n","#   encodings = tokenizer.encode_plus(\n","#       example,\n","#       None,\n","#       add_special_tokens=True,\n","#       max_length=MAX_LEN,\n","#       padding='max_length',\n","#       return_token_type_ids=True,\n","#       truncation=True,\n","#       return_attention_mask=True,\n","#       return_tensors='pt'\n","#   )\n","#   model.eval()\n","#   with torch.no_grad():\n","#       input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","#       attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","#       token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","#       output, _ = model(input_ids, attention_mask, token_type_ids)\n","#       final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n","#       # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n","#       # final=[0 if i<0.4 else 1 for i in final_output[0]]\n","#       # print(\"final\",final)\n","#       final_list.append(final_output[0])\n","\n","# def finalLabels2(predicted_list,val_list):\n","\n","#   indices=np.array(predicted_list).argsort()[::-1][:int(sum(val_list))]\n","#   for j in range(len(predicted_list)):\n","#     if j in indices:\n","#       predicted_list[j]=1.0\n","#     else:\n","#       predicted_list[j]=0.0\n","#   return predicted_list\n","\n","\n","# def finalLabels(predicted_list,val_list):\n","#   for i in range(len(predicted_list)):\n","#     indices=np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))]\n","#     # argsort()[:-1][:n]\n","#     # print(predicted_list,np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))])\n","#     for j in range(len(predicted_list[i])):\n","#       if j in indices:\n","#         predicted_list[i][j]=1.0\n","#       else:\n","#         predicted_list[i][j]=0.0\n","#   return predicted_list\n","\n","\n","# final_list= []\n","# examples = []\n","# # for i in val_df['text']:\n","# #   example = i\n","# Final_Outputs = []\n","# for i in range(len(val_df)):\n","#   example  = val_df.loc[i]['Text']\n","#   target  = (val_df.loc[i][target_List]).tolist()\n","#   encodings = tokenizer.encode_plus(\n","#       example,\n","#       None,\n","#       add_special_tokens=True,\n","#       max_length=MAX_LEN,\n","#       padding='max_length',\n","#       return_token_type_ids=True,\n","#       truncation=True,\n","#       return_attention_mask=True,\n","#       return_tensors='pt'\n","#   )\n","#   model.eval()\n","#   with torch.no_grad():\n","#       input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","#       attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","#       token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","#       output, _ = model(input_ids, attention_mask, token_type_ids)\n","#       temp = torch.Tensor(target).type(torch.int64).to(device)\n","#       loss, reservation = loss_fn(output,temp.reshape([1,dimension]))\n","#       final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n","#       # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n","#       # final=[0 if i<0.4 else 1 for i in final_output[0]]\n","#       # print(\"final\",final)\n","#       # print(final_output[0][:-1], target)\n","#       temp = finalLabels2(final_output[0][:-1],target)\n","#       # print(temp)\n","\n","#       # raise KeyInterruption\n","#       # print(\"tepm: \",temp)\n","#       # print(\"target:\", target.tolist())\n","#       # print(\"All: \", temp+target+torch.Tensor.tolist(reservation))\n","\n","#       final_list.append(temp+target+torch.Tensor.tolist(reservation))\n","#       examples.append(example)\n","#       Final_Outputs.append({'text':example,'target':target, 'output':temp, 'reservation':reservation})\n","#       # record\n","\n","\n","\n","# from operator import itemgetter\n","# Final_Outputs.sort(key=itemgetter('reservation'),reverse = False)\n","# # print(dimension,2*dimension-1)\n","# # print(\"final first cell:\", final_list[0])\n","\n","# sorted_final_list = sorted(final_list, key=itemgetter(2*dimension - 1), reverse = False)\n","# # print(sorted_final_list)\n","# # print(Final_Outputs[:2])\n","# # import pickle\n","# # with open(\"Final_Outputs\", \"wb\") as fp:\n","# #   pickle.dump(Final_Outputs, fp)\n","\n","# # with open(\"Final_Outputs\", \"rb\") as fp:\n","# #   Final_Outputs = pickle.load(fp)\n","\n","# from sklearn.metrics import multilabel_confusion_matrix\n","\n","# def get_accuracies(true_labels, predictions):\n","#     #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n","#     cm = multilabel_confusion_matrix(true_labels, predictions)\n","#     total_count = np.array(true_labels).shape[0]\n","#     accuracies = []\n","#     for i in range(np.array(true_labels).shape[1]):\n","#         true_positive_count = np.sum(cm[i,1,1]).item()\n","#         true_negative_count = np.sum(cm[i,0,0]).item()\n","#         accuracy = (true_positive_count + true_negative_count) / total_count\n","#         accuracies.append(accuracy)\n","#     return accuracies\n","\n","\n","\n","\n","# from sklearn.metrics import classification_report\n","# from sklearn.metrics import matthews_corrcoef\n","# # import tensorflow_addons as tfa\n","\n","# label_names = target_List\n","# for threshold in [1, 0.95, .9, 0.85, 0.8, .75]: #,\n","#   list_data = sorted_final_list[round(threshold*len(sorted_final_list)-1):]\n","#   list_data = sorted_final_list[:round(threshold*len(sorted_final_list)-1)]\n","#   val_list = [list_data[i][4:8] for i in range(len(list_data))]\n","#   prediction = [list_data[i][0:4] for i in range(len(list_data))]\n","#   print('############# '+TheClassifier_Abstract+'_'+str(rand_state)+'_'+str(threshold)+'   #############')\n","\n","#   #  print(\"val_list\",val_list[0:5])\n","#   #  print(\"prediction\", prediction[0:5])\n","#   #  print(\"label_names\", label_names)\n","#   print(classification_report(np.argmax(val_list, axis=1), np.argmax(prediction, axis=1), target_names=label_names))\n","#   accuracies = get_accuracies(val_list,prediction)\n","#   accuracy_average = sum(accuracies)/dimension\n","#   accuracies = [round(accuracies[i],2) for i in range(dimension)]\n","\n","#   print(\"accuracies for each class:\",accuracies, \"Average:\", accuracy_average)\n","\n","#   # print(len(val_list), val_list[0])\n","#   # print(len(prediction), prediction[0])\n","\n","\n","#   val = pd.DataFrame(val_list, columns = target_List)\n","#   fin = pd.DataFrame(prediction, columns = target_List)\n","\n","#   print(\"MCC [Aspect1, Aspect2, Aspect3, Aspect4]\", matthews_corrcoef(val[\"Aspect1\"],fin[\"Aspect1\"]), matthews_corrcoef(val[\"Aspect2\"],fin[\"Aspect2\"]), matthews_corrcoef(val[\"Aspect3\"],fin[\"Aspect3\"]), matthews_corrcoef(val[\"Aspect4\"],fin[\"Aspect4\"]))"]}]}