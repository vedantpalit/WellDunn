{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tensorflow_addons\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3_TQ_n6xc_OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71784a2c-533e-4653-d1a3-ccc0c6e6559f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import XLNetModel, XLNetTokenizer"
      ],
      "metadata": {
        "id": "S2iHkVqoc9Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kcf6BVFDuZy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egCTxAPncEaC"
      },
      "outputs": [],
      "source": [
        "data_new=pd.read_csv('/content/drive/MyDrive/Wellness_Dataset_Paper/DataFile.csv')\n",
        "data=pd.DataFrame()\n",
        "data['Text']=data_new['Text']\n",
        "data['Aspect']=data_new['Aspect']\n",
        "data['Aspect1']=data_new['Aspect']\n",
        "data['Aspect2']=data_new['Aspect']\n",
        "data['Aspect3']=data_new['Aspect']\n",
        "data['Aspect4']=data_new['Aspect']\n",
        "data['Explanations']=data_new['Explanations']\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i!=1:\n",
        "    data['Aspect1']=data['Aspect1'].replace(i,0)\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i==2:\n",
        "    data['Aspect2']=data['Aspect2'].replace(i,1)\n",
        "  else:\n",
        "    data['Aspect2']=data['Aspect2'].replace(i,0)\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i==3:\n",
        "    data['Aspect3']=data['Aspect3'].replace(i,1)\n",
        "  else:\n",
        "    data['Aspect3']=data['Aspect3'].replace(i,0)\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i==4:\n",
        "    data['Aspect4']=data['Aspect4'].replace(i,1)\n",
        "  else:\n",
        "    data['Aspect4']=data['Aspect4'].replace(i,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 64\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE= 1e-05"
      ],
      "metadata": {
        "id": "yt5cQStrcs6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = 4\n",
        "dimensions_list = [ 'Aspect1', 'Aspect2', 'Aspect3', 'Aspect4']\n",
        "\n",
        "# threshold = 1 #\n",
        "Classifiers = [\"nghuyong/ernie-2.0-en\", \"bert-base-uncased\",\"roberta-base\" ,\"emilyalsentzer/Bio_ClinicalBERT\", \"xlnet-base-cased\",'nlptown/bert-base-multilingual-uncased-sentiment', \"mental/mental-bert-base-uncased\"]\n",
        "Classifiers_Abs = [\"ERNIE\", \"BERT\", \"RoBERTa\", \"ClinicalBERT\", \"XLNET\", \"PsychBERT\", \"Mental-BERT\"]\n",
        "classifier_index = 3 #set as needed\n",
        "TheClassifier = Classifiers[classifier_index]\n",
        "TheClassifier_Abstract = Classifiers_Abs[classifier_index]"
      ],
      "metadata": {
        "id": "YThJ7WGVcuhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_states = [200, 345, 546, 678]\n",
        "ran_index = 2\n",
        "rand_state = rand_states[ran_index]\n",
        "\n",
        "target_List = dimensions_list"
      ],
      "metadata": {
        "id": "W5xOcNxxc15u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Text']\n",
        "        self.targets = self.df[target_List].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # print(index,\":\",self.title[index])\n",
        "        # print(self.title[index])\n",
        "        title = self.title[index]\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ],
      "metadata": {
        "id": "ySRbwDLTc7NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "6HydpBEDu5t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if classifier_index==3:\n",
        "  tokenizer=AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "  class ClinicalBIGBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClinicalBIGBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "        self.model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = ClinicalBIGBERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if classifier_index==4:\n",
        "  tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "  class XLNETClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XLNETClass, self).__init__()\n",
        "        self.xlnet_model = XLNetModel.from_pretrained(\"xlnet-base-cased\", output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension + 1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output_dict = self.xlnet_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        last_hidden_state = output_dict.last_hidden_state\n",
        "        attention_weights = output_dict.attentions\n",
        "        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, attention_weights[-1]\n",
        "\n",
        "  model = XLNETClass()\n",
        "  model.to(device)\n",
        "elif classifier_index==5:\n",
        "  tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  class PsychBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PsychBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "        self.model = AutoModel.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768,dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = PsychBERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "elif classifier_index==6:\n",
        "  !huggingface-cli login\n",
        "  tokenizer=AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased',use_auth_token=True)\n",
        "  class MentalBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MentalBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased')\n",
        "        self.model = AutoModel.from_pretrained('mental/mental-bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = MentalBERTClass()\n",
        "  model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "pipRUP_1dCrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeaaba33-5642-43ea-c466-b56417a50b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.2\n",
        "val_df = data.sample(frac=test_size, random_state=rand_state).reset_index (drop=True)\n",
        "train_df = data.drop (val_df.index).reset_index (drop=True)\n",
        "\n",
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "train_dataset=CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset=CustomDataset(val_df, tokenizer,MAX_LEN)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader (\n",
        "train_dataset,\n",
        "shuffle=True,\n",
        "batch_size=TRAIN_BATCH_SIZE,\n",
        "num_workers=0\n",
        ")\n",
        "val_data_loader = torch.utils.data.DataLoader (\n",
        "valid_dataset,\n",
        "shuffle=True,\n",
        "batch_size=VALID_BATCH_SIZE,\n",
        "num_workers=0\n",
        ")\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into\n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    print(\"checkpoint_path:\",checkpoint_path)\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "metadata": {
        "id": "iFmrgIHwdbJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(m_outputs, targets):\n",
        "        reward = 4\n",
        "\n",
        "        tensor_temp = torch.zeros(32,dtype=torch.float)\n",
        "        tensor_temp.to(device)\n",
        "        outputs = torch.nn.functional.softmax(m_outputs, dim=1,dtype=torch.float)\n",
        "\n",
        "        outputs, reservation = outputs[:, :-1], outputs[:, -1]\n",
        "\n",
        "        # gain = torch.gather(outputs, dim=1, index=targets).squeeze()\n",
        "        # print(\"targets:\",targets)\n",
        "        # print(\"outputs:\", outputs)\n",
        "        # raise KeyboardInterrupt\n",
        "        # return targets, outputs\n",
        "        gain = torch.einsum(\"ij, ij -> i\", targets.to(torch.float), outputs)\n",
        "\n",
        "        # doubling_rate = (gain.max() + reservation / reward).log()\n",
        "        doubling_rate = -torch.log(gain + reservation/reward)\n",
        "        return  doubling_rate.mean(), reservation"
      ],
      "metadata": {
        "id": "uiI-kUh-eBXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model,\n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "\n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        # print(data['input_ids'])\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        # print(ids)\n",
        "        # raise KeyboardInterrupt\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs, _ = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, reservationn = loss_fn(outputs, targets.type(torch.int64))\n",
        "\n",
        "        # print(outputs)\n",
        "        # loss2 = loss_fn2(outputs, targets)\n",
        "\n",
        "        # print(\"loss gambler: \",loss)\n",
        "        # print(\"reservation: \", reservationn)\n",
        "\n",
        "        # print(\"loss2 CE: \", loss2)\n",
        "\n",
        "        # raise KeyboardInterrupt\n",
        "        # tar, outp = loss_fn(outputs, targets.type(torch.int64))\n",
        "        # return tar, outp\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "\n",
        "    # print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "\n",
        "    # print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs, _ = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss, _ = loss_fn(outputs, targets.type(torch.int64))\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      # print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics\n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch,\n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        # save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    # print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "UV7y0vXgeF-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, sys\n",
        "\n",
        "trained_model = train_model(5, train_data_loader, val_data_loader, model, optimizer, \"ckpt_path/themodel3.pt\", \"thebestone3.pt\")"
      ],
      "metadata": {
        "id": "BGCa1JepeHyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d9c89a-97ff-41d0-b6fa-82a82c8d8fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tAvgerage Training Loss: 0.000823 \tAverage Validation Loss: 0.003176\n",
            "Validation loss decreased (inf --> 0.003176).  Saving model ...\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.000548 \tAverage Validation Loss: 0.002385\n",
            "Validation loss decreased (0.003176 --> 0.002385).  Saving model ...\n",
            "Epoch: 3 \tAvgerage Training Loss: 0.000404 \tAverage Validation Loss: 0.002258\n",
            "Validation loss decreased (0.002385 --> 0.002258).  Saving model ...\n",
            "Epoch: 4 \tAvgerage Training Loss: 0.000266 \tAverage Validation Loss: 0.002302\n",
            "Epoch: 5 \tAvgerage Training Loss: 0.000181 \tAverage Validation Loss: 0.001322\n",
            "Validation loss decreased (0.002258 --> 0.001322).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer_attentions = []\n",
        "for batch_idx, data in enumerate(train_data_loader):\n",
        "    ids = data['input_ids'].to(device, dtype=torch.long)\n",
        "    mask = data['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "    outputs, output_with_attention = model(ids, mask, token_type_ids)\n",
        "\n",
        "    if classifier_index == 4:\n",
        "        attentions = output_with_attention  # For XLNET\n",
        "    else:\n",
        "        attentions = output_with_attention.attentions[0]\n",
        "\n",
        "    for sample in attentions:\n",
        "        last_layer_attentions.append((sample[11]).detach().cpu())  # Detach and move to CPU\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del ids, mask, token_type_ids, outputs, output_with_attention\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "lry2SJVaeOG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import svd\n",
        "from numpy.linalg import matrix_rank\n",
        "\n",
        "d=[item.detach().numpy() for item in last_layer_attentions]\n",
        "U, S, VT = svd(d)\n",
        "\n",
        "print('*********************************************')\n",
        "# print(\"Experiment:\", dimension)\n",
        "print(\"SVD_ranking:\", matrix_rank(S))"
      ],
      "metadata": {
        "id": "hk1rR86VeQLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7d9c72-7490-4c1c-d5a2-cabe82180399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*********************************************\n",
            "SVD_ranking: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_list=[]\n",
        "for i in val_df['Text']:\n",
        "  example = i\n",
        "  encodings = tokenizer.encode_plus(\n",
        "      example,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "      output, _ = model(input_ids, attention_mask, token_type_ids)\n",
        "      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
        "      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n",
        "      # print(\"final\",final)\n",
        "      final_list.append(final_output[0])\n"
      ],
      "metadata": {
        "id": "q0pYWD1_eSHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finalLabels2(predicted_list,val_list):\n",
        "\n",
        "  indices=np.array(predicted_list).argsort()[::-1][:int(sum(val_list))]\n",
        "  for j in range(len(predicted_list)):\n",
        "    if j in indices:\n",
        "      predicted_list[j]=1.0\n",
        "    else:\n",
        "      predicted_list[j]=0.0\n",
        "  return predicted_list\n",
        "\n",
        "\n",
        "def finalLabels(predicted_list,val_list):\n",
        "  for i in range(len(predicted_list)):\n",
        "    indices=np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))]\n",
        "    # argsort()[:-1][:n]\n",
        "    # print(predicted_list,np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))])\n",
        "    for j in range(len(predicted_list[i])):\n",
        "      if j in indices:\n",
        "        predicted_list[i][j]=1.0\n",
        "      else:\n",
        "        predicted_list[i][j]=0.0\n",
        "  return predicted_list\n",
        "\n",
        "\n",
        "final_list= []\n",
        "examples = []\n",
        "# for i in val_df['text']:\n",
        "#   example = i\n",
        "Final_Outputs = []\n",
        "for i in range(len(val_df)):\n",
        "  example  = val_df.loc[i]['Text']\n",
        "  target  = (val_df.loc[i][target_List]).tolist()\n",
        "  encodings = tokenizer.encode_plus(\n",
        "      example,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "      output, _ = model(input_ids, attention_mask, token_type_ids)\n",
        "      temp = torch.Tensor(target).type(torch.int64).to(device)\n",
        "      loss, reservation = loss_fn(output,temp.reshape([1,dimension]))\n",
        "      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
        "      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n",
        "      # print(\"final\",final)\n",
        "      # print(final_output[0][:-1], target)\n",
        "      temp = finalLabels2(final_output[0][:-1],target)\n",
        "      # print(temp)\n",
        "\n",
        "      # raise KeyInterruption\n",
        "      # print(\"tepm: \",temp)\n",
        "      # print(\"target:\", target.tolist())\n",
        "      # print(\"All: \", temp+target+torch.Tensor.tolist(reservation))\n",
        "\n",
        "      final_list.append(temp+target+torch.Tensor.tolist(reservation))\n",
        "      examples.append(example)\n",
        "      Final_Outputs.append({'text':example,'target':target, 'output':temp, 'reservation':reservation})\n",
        "      # record\n",
        "\n",
        "\n",
        "\n",
        "from operator import itemgetter\n",
        "Final_Outputs.sort(key=itemgetter('reservation'),reverse = False)\n",
        "# print(dimension,2*dimension-1)\n",
        "# print(\"final first cell:\", final_list[0])\n",
        "\n",
        "sorted_final_list = sorted(final_list, key=itemgetter(2*dimension - 1), reverse = False)\n",
        "# print(sorted_final_list)\n",
        "# print(Final_Outputs[:2])\n",
        "# import pickle\n",
        "# with open(\"Final_Outputs\", \"wb\") as fp:\n",
        "#   pickle.dump(Final_Outputs, fp)\n",
        "\n",
        "# with open(\"Final_Outputs\", \"rb\") as fp:\n",
        "#   Final_Outputs = pickle.load(fp)\n",
        "\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "def get_accuracies(true_labels, predictions):\n",
        "    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
        "    cm = multilabel_confusion_matrix(true_labels, predictions)\n",
        "    total_count = np.array(true_labels).shape[0]\n",
        "    accuracies = []\n",
        "    for i in range(np.array(true_labels).shape[1]):\n",
        "        true_positive_count = np.sum(cm[i,1,1]).item()\n",
        "        true_negative_count = np.sum(cm[i,0,0]).item()\n",
        "        accuracy = (true_positive_count + true_negative_count) / total_count\n",
        "        accuracies.append(accuracy)\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "# import tensorflow_addons as tfa\n",
        "\n",
        "label_names = target_List\n",
        "for threshold in [1, 0.95, .9, 0.85, 0.8, .75]: #,\n",
        "  list_data = sorted_final_list[round(threshold*len(sorted_final_list)-1):]\n",
        "  list_data = sorted_final_list[:round(threshold*len(sorted_final_list)-1)]\n",
        "  val_list = [list_data[i][4:8] for i in range(len(list_data))]\n",
        "  prediction = [list_data[i][0:4] for i in range(len(list_data))]\n",
        "  print('############# '+TheClassifier_Abstract+'_'+str(rand_state)+'_'+str(threshold)+'   #############')\n",
        "\n",
        "  #  print(\"val_list\",val_list[0:5])\n",
        "  #  print(\"prediction\", prediction[0:5])\n",
        "  #  print(\"label_names\", label_names)\n",
        "  print(classification_report(np.argmax(val_list, axis=1), np.argmax(prediction, axis=1), target_names=label_names))\n",
        "  accuracies = get_accuracies(val_list,prediction)\n",
        "  accuracy_average = sum(accuracies)/dimension\n",
        "  accuracies = [round(accuracies[i],2) for i in range(dimension)]\n",
        "\n",
        "  print(\"accuracies for each class:\",accuracies, \"Average:\", accuracy_average)\n",
        "\n",
        "  # print(len(val_list), val_list[0])\n",
        "  # print(len(prediction), prediction[0])\n",
        "\n",
        "\n",
        "  val = pd.DataFrame(val_list, columns = target_List)\n",
        "  fin = pd.DataFrame(prediction, columns = target_List)\n",
        "\n",
        "  print(\"MCC [Aspect1, Aspect2, Aspect3, Aspect4]\", matthews_corrcoef(val[\"Aspect1\"],fin[\"Aspect1\"]), matthews_corrcoef(val[\"Aspect2\"],fin[\"Aspect2\"]), matthews_corrcoef(val[\"Aspect3\"],fin[\"Aspect3\"]), matthews_corrcoef(val[\"Aspect4\"],fin[\"Aspect4\"]))\n"
      ],
      "metadata": {
        "id": "mBX8tvEteWTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dccd05-2c2e-4f14-a521-692b8643203b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# XLNET_546_1   #############\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.91      0.62      0.74       165\n",
            "     Aspect2       0.83      0.97      0.90       106\n",
            "     Aspect3       0.94      0.99      0.96       242\n",
            "     Aspect4       0.76      0.92      0.83       104\n",
            "\n",
            "    accuracy                           0.88       617\n",
            "   macro avg       0.86      0.88      0.86       617\n",
            "weighted avg       0.88      0.88      0.87       617\n",
            "\n",
            "accuracies for each class: [0.88, 0.96, 0.97, 0.94] Average: 0.9376012965964344\n",
            "MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.6844550864636921 0.8759779332844609 0.9369181023086245 0.802903320914165\n",
            "############# XLNET_546_0.95   #############\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.94      0.62      0.75       165\n",
            "     Aspect2       0.83      0.97      0.90       106\n",
            "     Aspect3       0.94      0.99      0.96       242\n",
            "     Aspect4       0.70      0.96      0.81        73\n",
            "\n",
            "    accuracy                           0.88       586\n",
            "   macro avg       0.85      0.88      0.85       586\n",
            "weighted avg       0.89      0.88      0.87       586\n",
            "\n",
            "accuracies for each class: [0.88, 0.96, 0.97, 0.94] Average: 0.9385665529010239\n",
            "MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.7005575597662129 0.8745112766943508 0.9379342163781034 0.7904043444932783\n",
            "############# XLNET_546_0.9   #############\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.97      0.62      0.76       165\n",
            "     Aspect2       0.83      0.97      0.90       106\n",
            "     Aspect3       0.94      0.99      0.96       242\n",
            "     Aspect4       0.58      1.00      0.74        42\n",
            "\n",
            "    accuracy                           0.88       555\n",
            "   macro avg       0.83      0.89      0.84       555\n",
            "weighted avg       0.90      0.88      0.87       555\n",
            "\n",
            "accuracies for each class: [0.88, 0.96, 0.97, 0.95] Average: 0.9378378378378378\n",
            "MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.7124453814703423 0.8728393619427808 0.9353189239433864 0.7410939703603608\n",
            "############# XLNET_546_0.85   #############\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.97      0.62      0.76       165\n",
            "     Aspect2       0.83      0.97      0.90       106\n",
            "     Aspect3       0.94      0.99      0.96       242\n",
            "     Aspect4       0.27      1.00      0.42        11\n",
            "\n",
            "    accuracy                           0.87       524\n",
            "   macro avg       0.75      0.89      0.76       524\n",
            "weighted avg       0.91      0.87      0.87       524\n",
            "\n",
            "accuracies for each class: [0.87, 0.95, 0.97, 0.94] Average: 0.9341603053435115\n",
            "MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.7076083575289297 0.8709158497682069 0.9321199830092263 0.5025963115128412\n",
            "############# XLNET_546_0.8   #############\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.97      0.63      0.76       155\n",
            "     Aspect2       0.83      0.97      0.89       103\n",
            "     Aspect3       0.95      0.99      0.97       235\n",
            "     Aspect4       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87       493\n",
            "   macro avg       0.69      0.65      0.66       493\n",
            "weighted avg       0.93      0.87      0.89       493\n",
            "\n",
            "accuracies for each class: [0.88, 0.95, 0.97, 0.95] Average: 0.9350912778904664\n",
            "MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.712302247750576 0.8663017328642715 0.9358436868310592 0.0\n",
            "############# XLNET_546_0.75   #############\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.97      0.64      0.77       148\n",
            "     Aspect2       0.82      0.97      0.89        94\n",
            "     Aspect3       0.94      0.99      0.96       220\n",
            "     Aspect4       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87       462\n",
            "   macro avg       0.68      0.65      0.65       462\n",
            "weighted avg       0.93      0.87      0.89       462\n",
            "\n",
            "accuracies for each class: [0.88, 0.95, 0.97, 0.95] Average: 0.935064935064935\n",
            "MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.7167157179330613 0.8609821206915546 0.931595336979657 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}