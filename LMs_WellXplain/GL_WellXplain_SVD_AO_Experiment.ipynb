{"cells":[{"cell_type":"markdown","metadata":{"id":"f1QBr6zJLjjD"},"source":["# This code is for Gambler loss scenario on WellXplain.csv dataset.\n"," How to run the code?\n","\n","1) To run the code first you need to upload the WellXplain.csv into the path this code is. Also you need to assign use_auth_token variable with the huggingface token to access the models. To get this access it may take few hours when you request it in your huggingface account. So you need to request it in advance before runing the code.\n","2) Set classifier_index variable which represents what method (model) are you going to consider.\n","3) Set target_index varibale which represents what dimension you want to run the code on. It could be 0 for 6-dimension, 1 for 5-dimension, and 2 for 4-dimension.\n","4) run the code now in the order the cells appear. note that the possible values for each variable is provided in the line the the variable is assigned.\n","5) We used A100 GPU on Google Colab for runing this code. We tried **Colab Pro**"]},{"cell_type":"code","execution_count":138,"metadata":{"id":"yt5cQStrcs6a","executionInfo":{"status":"ok","timestamp":1718495037498,"user_tz":240,"elapsed":378,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["# We tried three different random_state for sampling data: 200, 345, and 546.\n","# It sets 200 as a defualt. You can change it in this cell if you want.\n","\n","rand_state=200\n","classifier_index = 0 #Models [0:\"ERNIE\", 1:\"BERT\", 2:\"RoBERTa\", 3:\"ClinicalBERT\", 4:\"XLNET\", 5:\"PsychBERT\", 6:\"Mental-BERT\"]\n","use_auth_token = 'hf_...' # You have to assign your huggingface access token here. We removed our own token for any identity violation\n","#Do Not change the following hyperparameters\n","dimension=4\n","MAX_LEN = 64\n","TRAIN_BATCH_SIZE = 32 # for models except XLNET and MentalBERT it is 32, else it is 2 (due to memory issue)\n","VALID_BATCH_SIZE = 32 # for models except XLNET and MentalBERT it is 32, else it is 2 (due to memory issue)\n","EPOCHS = 5\n","LEARNING_RATE= 1e-05"]},{"cell_type":"code","execution_count":139,"metadata":{"id":"YThJ7WGVcuhi","executionInfo":{"status":"ok","timestamp":1718495037871,"user_tz":240,"elapsed":3,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["dimensions_list = [ 'Aspect1', 'Aspect2', 'Aspect3', 'Aspect4']\n","\n","# threshold = 1 #\n","Classifiers = [\"nghuyong/ernie-2.0-en\", \"bert-base-uncased\",\"roberta-base\" ,\"emilyalsentzer/Bio_ClinicalBERT\", \"xlnet-base-cased\",'nlptown/bert-base-multilingual-uncased-sentiment', \"mental/mental-bert-base-uncased\"]\n","Classifiers_Abs = [\"ERNIE\", \"BERT\", \"RoBERTa\", \"ClinicalBERT\", \"XLNET\", \"PsychBERT\", \"Mental-BERT\"]\n","\n","TheClassifier = Classifiers[classifier_index]\n","TheClassifier_Abstract = Classifiers_Abs[classifier_index]\n","target_List = dimensions_list"]},{"cell_type":"markdown","metadata":{"id":"I0A7Y2UBK8G0"},"source":["#Install and Utils"]},{"cell_type":"markdown","metadata":{"id":"W3zhBqyOL9f-"},"source":["In this section, requirements like numpy, pandas libraries are imported. Also, the pretrained models that we are using in this code are downloaded in this section.\n","\n","Action required: you may need to log in into hugginface for usage of MentalBERT"]},{"cell_type":"code","execution_count":140,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_TQ_n6xc_OD","outputId":"d059de14-4a5b-435d-b6a2-b6c617765c81","executionInfo":{"status":"ok","timestamp":1718495053414,"user_tz":240,"elapsed":15545,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.1)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"]}],"source":["!pip install sentencepiece\n","!pip install tensorflow_addons\n","!pip install transformers"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"S2iHkVqoc9Qk","executionInfo":{"status":"ok","timestamp":1718495053414,"user_tz":240,"elapsed":4,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["from transformers import AutoModel, AutoTokenizer\n","from transformers import XLNetModel, XLNetTokenizer"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"kcf6BVFDuZy7","executionInfo":{"status":"ok","timestamp":1718495053414,"user_tz":240,"elapsed":3,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"w25XUM1kMIa8"},"source":["#Dataset Prep"]},{"cell_type":"markdown","metadata":{"id":"BFUK-FgIMLku"},"source":["**Action required**:\n","\n","Before runing this cell, you will need to move WellXplain.csv file into the path that this code is. On Google colab, you can upload WellXplain.csv file into Session Storage."]},{"cell_type":"code","execution_count":143,"metadata":{"id":"cP4mWefeFo_O","executionInfo":{"status":"ok","timestamp":1718495053895,"user_tz":240,"elapsed":484,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["from huggingface_hub import login\n","if classifier_index==6:\n","  login(token=use_auth_token)\n","tokenizer = AutoTokenizer.from_pretrained(TheClassifier)"]},{"cell_type":"code","execution_count":144,"metadata":{"id":"6HydpBEDu5t6","executionInfo":{"status":"ok","timestamp":1718495053895,"user_tz":240,"elapsed":4,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #set as cpu if gpu unavailable"]},{"cell_type":"code","execution_count":145,"metadata":{"id":"egCTxAPncEaC","executionInfo":{"status":"ok","timestamp":1718495053896,"user_tz":240,"elapsed":4,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["data_new=pd.read_csv('WellXplain.csv')\n","data=pd.DataFrame()\n","data['Text']=data_new['Text']\n","data['Aspect']=data_new['Aspect']\n","data['Aspect1']=data_new['Aspect']\n","data['Aspect2']=data_new['Aspect']\n","data['Aspect3']=data_new['Aspect']\n","data['Aspect4']=data_new['Aspect']\n","data['Explanations']=data_new['Explanations']\n","\n","for i in range(1,5):\n","  if i!=1:\n","    data['Aspect1']=data['Aspect1'].replace(i,0)\n","\n","for i in range(1,5):\n","  if i==2:\n","    data['Aspect2']=data['Aspect2'].replace(i,1)\n","  else:\n","    data['Aspect2']=data['Aspect2'].replace(i,0)\n","\n","for i in range(1,5):\n","  if i==3:\n","    data['Aspect3']=data['Aspect3'].replace(i,1)\n","  else:\n","    data['Aspect3']=data['Aspect3'].replace(i,0)\n","\n","for i in range(1,5):\n","  if i==4:\n","    data['Aspect4']=data['Aspect4'].replace(i,1)\n","  else:\n","    data['Aspect4']=data['Aspect4'].replace(i,0)"]},{"cell_type":"code","execution_count":146,"metadata":{"id":"ySRbwDLTc7NR","executionInfo":{"status":"ok","timestamp":1718495053896,"user_tz":240,"elapsed":3,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["import torch\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self, df, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = df['Text']\n","        self.targets = self.df[target_List].values\n","        self.max_len = max_len\n","\n","  def __len__(self):\n","        return len(self.title)\n","\n","  def __getitem__(self, index):\n","        # print(index,\":\",self.title[index])\n","        # print(self.title[index])\n","        title = self.title[index]\n","        title = \" \".join(title.split())\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index])\n","        }"]},{"cell_type":"code","execution_count":147,"metadata":{"id":"yRiZDXhUCjWF","executionInfo":{"status":"ok","timestamp":1718495053896,"user_tz":240,"elapsed":3,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["test_size = 0.2\n","val_df = data.sample(frac=test_size, random_state=rand_state).reset_index (drop=True)\n","train_df = data.drop (val_df.index).reset_index (drop=True) #preparing the train and test dataset\n","\n","\n","train_dataset=CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset=CustomDataset(val_df, tokenizer,MAX_LEN)\n","\n","train_data_loader = torch.utils.data.DataLoader (\n","train_dataset,\n","shuffle=True,\n","batch_size=TRAIN_BATCH_SIZE,\n","num_workers=0\n",")\n","val_data_loader = torch.utils.data.DataLoader (\n","valid_dataset,\n","shuffle=True,\n","batch_size=VALID_BATCH_SIZE,\n","num_workers=0\n",")"]},{"cell_type":"markdown","metadata":{"id":"DsVtsbBnMS_e"},"source":["#Model Selection and Model Save/Load"]},{"cell_type":"markdown","metadata":{"id":"Wxz_-y8tNF14"},"source":["For choosing the model, select the index from the list"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"iFmrgIHwdbJE","executionInfo":{"status":"ok","timestamp":1718495053896,"user_tz":240,"elapsed":3,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["# Functions for saving and loading the model in the case the training\n","# is interrupted. In this case, we use these functions start training\n","# again from last check point.\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into\n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss\n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n","\n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    print(\"checkpoint_path:\",checkpoint_path)\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        # copy that checkpoint file to best path given, best_model_path\n","        shutil.copyfile(f_path, best_fpath)"]},{"cell_type":"code","execution_count":149,"metadata":{"id":"Pj2V3aPKJ7fI","executionInfo":{"status":"ok","timestamp":1718495056742,"user_tz":240,"elapsed":2849,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["if classifier_index==0:\n","  tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-2.0-en')\n","  class ERNIEClass(torch.nn.Module):\n","    def __init__(self):\n","        super(ERNIEClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-2.0-en')\n","        self.ernie_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-en', output_hidden_states=True,output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, token_type_ids):\n","        output_dict = self.ernie_model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=token_type_ids,\n","            output_hidden_states=True,\n","            return_dict=True\n","        )\n","        last_hidden_state = output_dict.last_hidden_state\n","        attention_weights = output_dict.attentions\n","        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n","        output = self.linear(output_dropout)\n","        return output, attention_weights[-1]\n","\n","  model = ERNIEClass()\n","  model.to(device)\n","\n","if classifier_index==1:\n","  tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n","  class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = AutoModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","\n","\n","  model = BERTClass()\n","  model.to(device)\n","\n","if classifier_index==2:\n","  tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n","  class roBERTaClass(torch.nn.Module):\n","    def __init__(self):\n","        super(roBERTaClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","        self.model = AutoModel.from_pretrained('roberta-base',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","\n","\n","\n","\n","  model = roBERTaClass()\n","  model.to(device)\n"]},{"cell_type":"code","execution_count":150,"metadata":{"id":"pipRUP_1dCrJ","executionInfo":{"status":"ok","timestamp":1718495056742,"user_tz":240,"elapsed":2,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["if classifier_index==3:\n","  tokenizer=AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","  class ClinicalBIGBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(ClinicalBIGBERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","        self.model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","  model = ClinicalBIGBERTClass()\n","  model.to(device)\n","\n","\n","if classifier_index==4:\n","  tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n","  class XLNETClass(torch.nn.Module):\n","    def __init__(self):\n","        super(XLNETClass, self).__init__()\n","        self.xlnet_model = XLNetModel.from_pretrained(\"xlnet-base-cased\", output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension + 1)\n","\n","    def forward(self, input_ids, attn_mask, token_type_ids):\n","        output_dict = self.xlnet_model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=token_type_ids,\n","            output_hidden_states=True,\n","            return_dict=True\n","        )\n","        last_hidden_state = output_dict.last_hidden_state\n","        attention_weights = output_dict.attentions\n","        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n","        output = self.linear(output_dropout)\n","        return output, attention_weights[-1]\n","\n","  model = XLNETClass()\n","  model.to(device)\n","elif classifier_index==5:\n","  tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n","  class PsychBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(PsychBERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n","        self.model = AutoModel.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768,dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.pooler_output)\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","  model = PsychBERTClass()\n","  model.to(device)\n","\n","elif classifier_index==6:\n","  # !huggingface-cli login\n","  from huggingface_hub import login\n","\n","  login(token=use_auth_token)\n","  tokenizer=AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased', use_auth_token=use_auth_token) # use_auth_token has to be assign to huggingface key to access the mental bert model\n","  class MentalBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(MentalBERTClass, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased')\n","        self.model = AutoModel.from_pretrained('mental/mental-bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.linear = torch.nn.Linear(768, dimension+1)\n","\n","    def forward(self, input_ids, attn_mask, seg_ids):\n","        output = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attn_mask,\n","            token_type_ids=seg_ids\n","        )\n","        output_with_attention = output\n","        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n","        output = self.linear(output_dropout)\n","        return output, output_with_attention\n","\n","  model = MentalBERTClass()\n","  model.to(device)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l_7Tlg8qDAmO"},"source":["#Loss Function - Gamblers"]},{"cell_type":"code","execution_count":151,"metadata":{"id":"uiI-kUh-eBXS","executionInfo":{"status":"ok","timestamp":1718495057131,"user_tz":240,"elapsed":390,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["#Gamblers Loss Function\n","\n","def loss_fn(m_outputs, targets):\n","        reward = 4\n","\n","        tensor_temp = torch.zeros(32,dtype=torch.float)\n","        tensor_temp.to(device)\n","        outputs = torch.nn.functional.softmax(m_outputs, dim=1,dtype=torch.float)\n","\n","        outputs, reservation = outputs[:, :-1], outputs[:, -1]\n","\n","        # gain = torch.gather(outputs, dim=1, index=targets).squeeze()\n","        # print(\"targets:\",targets)\n","        # print(\"outputs:\", outputs)\n","        # raise KeyboardInterrupt\n","        # return targets, outputs\n","        gain = torch.einsum(\"ij, ij -> i\", targets.to(torch.float), outputs)\n","\n","        # doubling_rate = (gain.max() + reservation / reward).log()\n","        doubling_rate = -torch.log(gain + reservation/reward)\n","        return  doubling_rate.mean(), reservation"]},{"cell_type":"markdown","metadata":{"id":"OlFv6xjqDFIW"},"source":["#Training"]},{"cell_type":"code","execution_count":152,"metadata":{"id":"UV7y0vXgeF-k","executionInfo":{"status":"ok","timestamp":1718495057132,"user_tz":240,"elapsed":3,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n","\n","\n","\n","val_targets=[]\n","val_outputs=[]\n","\n","def train_model(n_epochs, training_loader, validation_loader, model,\n","                optimizer, checkpoint_path, best_model_path):\n","\n","  # initialize tracker for minimum validation loss\n","  valid_loss_min = np.Inf\n","\n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0\n","    valid_loss = 0\n","\n","    model.train()\n","\n","    for batch_idx, data in enumerate(training_loader):\n","        # print(data['input_ids'])\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        # print(ids)\n","        # raise KeyboardInterrupt\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","\n","        outputs, _ = model(ids, mask, token_type_ids)\n","\n","        optimizer.zero_grad()\n","        loss, reservationn = loss_fn(outputs, targets.type(torch.int64))\n","\n","        # print(outputs)\n","        # loss2 = loss_fn2(outputs, targets)\n","\n","        # print(\"loss gambler: \",loss)\n","        # print(\"reservation: \", reservationn)\n","\n","        # print(\"loss2 CE: \", loss2)\n","\n","        # raise KeyboardInterrupt\n","        # tar, outp = loss_fn(outputs, targets.type(torch.int64))\n","        # return tar, outp\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # print('before loss data in training', loss.item(), train_loss)\n","        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","        #print('after loss data in training', loss.item(), train_loss)\n","\n","    # print('############# Epoch {}: Training End     #############'.format(epoch))\n","\n","    # print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################\n","    # validate the model #\n","    ######################\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(validation_loader, 0):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs, _ = model(ids, mask, token_type_ids)\n","\n","            loss, _ = loss_fn(outputs, targets.type(torch.int64))\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","            val_targets.extend(targets.cpu().detach().numpy().tolist())\n","            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","      # print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      # calculate average losses\n","      #print('before cal avg train loss', train_loss)\n","      train_loss = train_loss/len(training_loader)\n","      valid_loss = valid_loss/len(validation_loader)\n","      # print training/validation statistics\n","      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n","            epoch,\n","            train_loss,\n","            valid_loss\n","            ))\n","\n","      # create checkpoint variable and add important data\n","      checkpoint = {\n","            'epoch': epoch + 1,\n","            'valid_loss_min': valid_loss,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","      }\n","        # save checkpoint\n","      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","\n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n","        # save checkpoint as best model\n","        # save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","        valid_loss_min = valid_loss\n","\n","    # print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","\n","  return model\n"]},{"cell_type":"code","execution_count":153,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGCa1JepeHyM","outputId":"306596b6-bd93-481b-874c-e1bfc92348ee","executionInfo":{"status":"ok","timestamp":1718495098810,"user_tz":240,"elapsed":41681,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 \tAvgerage Training Loss: 0.012691 \tAverage Validation Loss: 0.041974\n","Validation loss decreased (inf --> 0.041974).  Saving model ...\n","Epoch: 2 \tAvgerage Training Loss: 0.007792 \tAverage Validation Loss: 0.030171\n","Validation loss decreased (0.041974 --> 0.030171).  Saving model ...\n","Epoch: 3 \tAvgerage Training Loss: 0.005875 \tAverage Validation Loss: 0.024321\n","Validation loss decreased (0.030171 --> 0.024321).  Saving model ...\n","Epoch: 4 \tAvgerage Training Loss: 0.004428 \tAverage Validation Loss: 0.023281\n","Validation loss decreased (0.024321 --> 0.023281).  Saving model ...\n","Epoch: 5 \tAvgerage Training Loss: 0.003137 \tAverage Validation Loss: 0.022753\n","Validation loss decreased (0.023281 --> 0.022753).  Saving model ...\n"]}],"source":["import shutil, sys\n","\n","trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, \"ckpt_path/themodel3.pt\", \"thebestone3.pt\")"]},{"cell_type":"markdown","metadata":{"id":"TM1pH5VAOXDi"},"source":["#Calculation of SVD Ranking"]},{"cell_type":"markdown","metadata":{"id":"6DCCZOknJE5F"},"source":["This piece of code is for calculating the singular value decomposition ranking for the models"]},{"cell_type":"code","execution_count":155,"metadata":{"id":"lry2SJVaeOG4","executionInfo":{"status":"ok","timestamp":1718495117379,"user_tz":240,"elapsed":4076,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["last_layer_attentions = []\n","for batch_idx, data in enumerate(train_data_loader):\n","    ids = data['input_ids'].to(device, dtype=torch.long)\n","    mask = data['attention_mask'].to(device, dtype=torch.long)\n","    token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","    outputs, output_with_attention = model(ids, mask, token_type_ids)\n","\n","    if classifier_index in [0,4]:\n","        attentions = output_with_attention  # For XLNET and Ernie\n","    else:\n","        attentions = output_with_attention.attentions[0]\n","\n","    for sample in attentions:\n","        last_layer_attentions.append((sample[11]).detach().cpu())  # Detach and move to CPU\n","\n","    # Clear GPU memory\n","    del ids, mask, token_type_ids, outputs, output_with_attention\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":156,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hk1rR86VeQLM","outputId":"1a55880b-bceb-4ce6-85b0-5ad0574c269e","executionInfo":{"status":"ok","timestamp":1718495121530,"user_tz":240,"elapsed":2926,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["*********************************************\n","SVD_ranking: 11\n"]}],"source":["from numpy.linalg import svd\n","from numpy.linalg import matrix_rank\n","\n","d=[item.detach().numpy() for item in last_layer_attentions]\n","U, S, VT = svd(d)\n","\n","print('*********************************************')\n","# print(\"Experiment:\", dimension)\n","print(\"SVD_ranking:\", matrix_rank(S))"]},{"cell_type":"markdown","metadata":{"id":"NthBANqyOgUA"},"source":["#Metrics"]},{"cell_type":"markdown","metadata":{"id":"MPBOxRwlOki0"},"source":["This code can be run at once, to obtain the performance metric values for different values of the threshold - 1,0.95,0.9,0.85,0.8,0.75"]},{"cell_type":"code","execution_count":157,"metadata":{"id":"q0pYWD1_eSHJ","executionInfo":{"status":"ok","timestamp":1718495131816,"user_tz":240,"elapsed":7779,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["final_list=[]\n","for i in val_df['Text']:\n","  example = i\n","  encodings = tokenizer.encode_plus(\n","      example,\n","      None,\n","      add_special_tokens=True,\n","      max_length=MAX_LEN,\n","      padding='max_length',\n","      return_token_type_ids=True,\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt'\n","  )\n","  model.eval()\n","  with torch.no_grad():\n","      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","      output, _ = model(input_ids, attention_mask, token_type_ids)\n","      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n","      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n","      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n","      # print(\"final\",final)\n","      final_list.append(final_output[0])\n"]},{"cell_type":"code","execution_count":158,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBX8tvEteWTH","outputId":"88725112-6234-47cd-f4ac-f4368019af55","executionInfo":{"status":"ok","timestamp":1718495140715,"user_tz":240,"elapsed":8901,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["############# ERNIE_200_1   #############\n","              precision    recall  f1-score   support\n","\n","     Aspect1       0.90      0.52      0.66       144\n","     Aspect2       0.92      0.96      0.94       110\n","     Aspect3       0.90      0.99      0.94       233\n","     Aspect4       0.74      0.93      0.83       130\n","\n","    accuracy                           0.86       617\n","   macro avg       0.87      0.85      0.84       617\n","weighted avg       0.87      0.86      0.85       617\n","\n","accuracies for each class: [0.88, 0.98, 0.96, 0.92] Average: 0.9319286871961103\n","MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.6246888426073717 0.9296892918817031 0.9114384504556066 0.7811382985169307\n","############# ERNIE_200_0.95   #############\n","              precision    recall  f1-score   support\n","\n","     Aspect1       0.93      0.52      0.67       144\n","     Aspect2       0.92      0.96      0.94       110\n","     Aspect3       0.91      0.99      0.95       233\n","     Aspect4       0.69      0.95      0.80        99\n","\n","    accuracy                           0.86       586\n","   macro avg       0.86      0.86      0.84       586\n","weighted avg       0.88      0.86      0.85       586\n","\n","accuracies for each class: [0.87, 0.98, 0.96, 0.92] Average: 0.9317406143344711\n","MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.6327521991097605 0.9288543096979938 0.9147713575942077 0.7662037741606383\n","############# ERNIE_200_0.9   #############\n","              precision    recall  f1-score   support\n","\n","     Aspect1       0.94      0.52      0.67       144\n","     Aspect2       0.93      0.96      0.95       110\n","     Aspect3       0.91      0.99      0.95       233\n","     Aspect4       0.61      0.96      0.74        68\n","\n","    accuracy                           0.86       555\n","   macro avg       0.85      0.86      0.83       555\n","weighted avg       0.88      0.86      0.85       555\n","\n","accuracies for each class: [0.87, 0.98, 0.95, 0.92] Average: 0.9297297297297297\n","MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.6348113400656837 0.93312346856957 0.9113555211955262 0.7228156964918018\n","############# ERNIE_200_0.85   #############\n","              precision    recall  f1-score   support\n","\n","     Aspect1       0.96      0.52      0.68       144\n","     Aspect2       0.93      0.96      0.95       110\n","     Aspect3       0.91      0.99      0.95       233\n","     Aspect4       0.46      0.97      0.63        37\n","\n","    accuracy                           0.85       524\n","   macro avg       0.82      0.86      0.80       524\n","weighted avg       0.90      0.85      0.85       524\n","\n","accuracies for each class: [0.86, 0.98, 0.95, 0.92] Average: 0.9274809160305343\n","MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.6433150221530904 0.9321117572711618 0.9071952366846766 0.6381787565657538\n","############# ERNIE_200_0.8   #############\n","              precision    recall  f1-score   support\n","\n","     Aspect1       0.96      0.52      0.68       144\n","     Aspect2       0.93      0.96      0.95       110\n","     Aspect3       0.91      0.99      0.95       233\n","     Aspect4       0.12      1.00      0.22         6\n","\n","    accuracy                           0.85       493\n","   macro avg       0.73      0.87      0.70       493\n","weighted avg       0.92      0.85      0.86       493\n","\n","accuracies for each class: [0.85, 0.98, 0.95, 0.91] Average: 0.9239350912778905\n","MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.638254444141552 0.9309355415643145 0.9057549313871152 0.3379640698738068\n","############# ERNIE_200_0.75   #############\n","              precision    recall  f1-score   support\n","\n","     Aspect1       0.96      0.53      0.68       138\n","     Aspect2       0.93      0.96      0.94       104\n","     Aspect3       0.92      1.00      0.96       220\n","     Aspect4       0.00      0.00      0.00         0\n","\n","    accuracy                           0.85       462\n","   macro avg       0.70      0.62      0.65       462\n","weighted avg       0.93      0.85      0.87       462\n","\n","accuracies for each class: [0.85, 0.97, 0.96, 0.91] Average: 0.9242424242424243\n","MCC [Aspect1, Aspect2, Aspect3, Aspect4] 0.641632347606991 0.9268279064302561 0.9163227559370816 0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["def finalLabels2(predicted_list,val_list):\n","\n","  indices=np.array(predicted_list).argsort()[::-1][:int(sum(val_list))]\n","  for j in range(len(predicted_list)):\n","    if j in indices:\n","      predicted_list[j]=1.0\n","    else:\n","      predicted_list[j]=0.0\n","  return predicted_list\n","\n","\n","def finalLabels(predicted_list,val_list):\n","  for i in range(len(predicted_list)):\n","    indices=np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))]\n","    # argsort()[:-1][:n]\n","    # print(predicted_list,np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))])\n","    for j in range(len(predicted_list[i])):\n","      if j in indices:\n","        predicted_list[i][j]=1.0\n","      else:\n","        predicted_list[i][j]=0.0\n","  return predicted_list\n","\n","\n","final_list= []\n","examples = []\n","# for i in val_df['text']:\n","#   example = i\n","Final_Outputs = []\n","for i in range(len(val_df)):\n","  example  = val_df.loc[i]['Text']\n","  target  = (val_df.loc[i][target_List]).tolist()\n","  encodings = tokenizer.encode_plus(\n","      example,\n","      None,\n","      add_special_tokens=True,\n","      max_length=MAX_LEN,\n","      padding='max_length',\n","      return_token_type_ids=True,\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt'\n","  )\n","  model.eval()\n","  with torch.no_grad():\n","      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","      output, _ = model(input_ids, attention_mask, token_type_ids)\n","      temp = torch.Tensor(target).type(torch.int64).to(device)\n","      loss, reservation = loss_fn(output,temp.reshape([1,dimension]))\n","      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n","      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n","      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n","      # print(\"final\",final)\n","      # print(final_output[0][:-1], target)\n","      temp = finalLabels2(final_output[0][:-1],target)\n","      # print(temp)\n","\n","      # raise KeyInterruption\n","      # print(\"tepm: \",temp)\n","      # print(\"target:\", target.tolist())\n","      # print(\"All: \", temp+target+torch.Tensor.tolist(reservation))\n","\n","      final_list.append(temp+target+torch.Tensor.tolist(reservation))\n","      examples.append(example)\n","      Final_Outputs.append({'text':example,'target':target, 'output':temp, 'reservation':reservation})\n","      # record\n","\n","\n","\n","from operator import itemgetter\n","Final_Outputs.sort(key=itemgetter('reservation'),reverse = False)\n","# print(dimension,2*dimension-1)\n","# print(\"final first cell:\", final_list[0])\n","\n","sorted_final_list = sorted(final_list, key=itemgetter(2*dimension - 1), reverse = False)\n","# print(sorted_final_list)\n","# print(Final_Outputs[:2])\n","# import pickle\n","# with open(\"Final_Outputs\", \"wb\") as fp:\n","#   pickle.dump(Final_Outputs, fp)\n","\n","# with open(\"Final_Outputs\", \"rb\") as fp:\n","#   Final_Outputs = pickle.load(fp)\n","\n","from sklearn.metrics import multilabel_confusion_matrix\n","\n","def get_accuracies(true_labels, predictions):\n","    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n","    cm = multilabel_confusion_matrix(true_labels, predictions)\n","    total_count = np.array(true_labels).shape[0]\n","    accuracies = []\n","    for i in range(np.array(true_labels).shape[1]):\n","        true_positive_count = np.sum(cm[i,1,1]).item()\n","        true_negative_count = np.sum(cm[i,0,0]).item()\n","        accuracy = (true_positive_count + true_negative_count) / total_count\n","        accuracies.append(accuracy)\n","    return accuracies\n","\n","\n","\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import matthews_corrcoef\n","# import tensorflow_addons as tfa\n","\n","label_names = target_List\n","for threshold in [1, 0.95, .9, 0.85, 0.8, .75]: #,\n","  list_data = sorted_final_list[round(threshold*len(sorted_final_list)-1):]\n","  list_data = sorted_final_list[:round(threshold*len(sorted_final_list)-1)]\n","  val_list = [list_data[i][4:8] for i in range(len(list_data))]\n","  prediction = [list_data[i][0:4] for i in range(len(list_data))]\n","  print('############# '+TheClassifier_Abstract+'_'+str(rand_state)+'_'+str(threshold)+'   #############')\n","\n","  #  print(\"val_list\",val_list[0:5])\n","  #  print(\"prediction\", prediction[0:5])\n","  #  print(\"label_names\", label_names)\n","  print(classification_report(np.argmax(val_list, axis=1), np.argmax(prediction, axis=1), target_names=label_names))\n","  accuracies = get_accuracies(val_list,prediction)\n","  accuracy_average = sum(accuracies)/dimension\n","  accuracies = [round(accuracies[i],2) for i in range(dimension)]\n","\n","  print(\"accuracies for each class:\",accuracies, \"Average:\", accuracy_average)\n","\n","  # print(len(val_list), val_list[0])\n","  # print(len(prediction), prediction[0])\n","\n","\n","  val = pd.DataFrame(val_list, columns = target_List)\n","  fin = pd.DataFrame(prediction, columns = target_List)\n","\n","  print(\"MCC [Aspect1, Aspect2, Aspect3, Aspect4]\", matthews_corrcoef(val[\"Aspect1\"],fin[\"Aspect1\"]), matthews_corrcoef(val[\"Aspect2\"],fin[\"Aspect2\"]), matthews_corrcoef(val[\"Aspect3\"],fin[\"Aspect3\"]), matthews_corrcoef(val[\"Aspect4\"],fin[\"Aspect4\"]))\n"]},{"cell_type":"markdown","metadata":{"id":"hAaCP1zV4Bui"},"source":["# Attention Overlap score"]},{"cell_type":"code","execution_count":162,"metadata":{"id":"gfm8xIH24Eb0","executionInfo":{"status":"ok","timestamp":1718495320613,"user_tz":240,"elapsed":49013,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[],"source":["final_list=[]\n","last_layer_attentions = []\n","token_scores =[]\n","for i in val_df['Text']:\n","  example = i\n","  encodings = tokenizer.encode_plus(\n","      example,\n","      None,\n","      add_special_tokens=True,\n","      max_length=MAX_LEN,\n","      padding='max_length',\n","      return_token_type_ids=True,\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt'\n","  )\n","  model.eval()\n","  with torch.no_grad():\n","      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","      output,output_with_attention = model(input_ids, attention_mask, token_type_ids)\n","      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n","      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n","      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n","      # print(\"final\",final)\n","      input_id_list = input_ids[0].tolist()\n","      tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n","      # raise KeyBoardInteruupt\n","      #model_view(output_with_attention.attentions,tokens)\n","\n","      final_list.append(final_output[0])\n","      if classifier_index in [1,2,3,5]:\n","        attentions = output_with_attention.attentions[11][0][0]\n","      elif classifier_index==0:\n","        attentions = output_with_attention[0][11]\n","      elif classifier_index==4 or classifier_index==3:\n","        attentions = output_with_attention[0][11]\n","\n","      else:\n","        attentions = output_with_attention.attentions[0]\n","\n","      for sample in attentions:\n","        last_layer_attentions.append((sample[11]).detach().cpu())  # Detach and move to CPU\n","\n","\n","      # Calculate attention scores for each token\n","      attention_scores = torch.sum(attentions, dim=0)\n","      # Normalize attention scores\n","      normalized_scores = attention_scores / torch.sum(attention_scores)\n","\n","\n","\n","      # Associate each score with its corresponding token\n","      token_score = {}\n","      for j in range(len(normalized_scores)):\n","          token = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))[j]\n","          token_score[token] = normalized_scores[j].item()\n","\n","      token_scores.append(token_score)"]},{"cell_type":"code","execution_count":163,"metadata":{"id":"speQ9UEn4HLI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f115011e-77c2-42f2-8e8e-fb3cdfc7c2bb","executionInfo":{"status":"ok","timestamp":1718495320614,"user_tz":240,"elapsed":12,"user":{"displayName":"Ali Mohammadi","userId":"00057030456682161153"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["nghuyong/ernie-2.0-en\n","AO score: 0.23300970873786409\n","Number of samples with the ground truth explanations: 144.0 Number of total samples: 618\n"]}],"source":["from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","top = 4\n","Correct_samples = np.zeros((len(token_scores)))\n","for i in range(len(token_scores)):\n","  mytoken = token_scores[i]\n","  token_remove = []\n","  Explanation = (val_df.iloc[i]['Explanations']).split()\n","  for tok in mytoken.keys():\n","    if  tok in ['[SEP]', '[CLS]']:\n","      token_remove.append(tok)\n","  for item in token_remove:\n","    del mytoken[item]\n","  token_sorted = dict(sorted(mytoken.items(), key=lambda x:x[1], reverse=True))\n","\n","  token_sorted_top = dict(list(token_sorted.items())[0:top])\n","  # print(token_sorted_top)\n","  # raise KeyboardInterrupt\n","\n","  common_token = 0\n","  for item in Explanation:\n","    if item in token_sorted_top.keys():\n","      common_token += 1\n","  # print(Explanation, token_sorted_top, common_token)\n","  if common_token/len(Explanation)>.5:\n","    Correct_samples[i] = 1\n","  # raise KeyboardInterrupt\n","print(TheClassifier)\n","print('AO score:', sum(Correct_samples)/len(token_scores))\n","print('Number of samples with the ground truth explanations:',sum(Correct_samples),'Number of total samples:', len(token_scores))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["I0A7Y2UBK8G0"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}