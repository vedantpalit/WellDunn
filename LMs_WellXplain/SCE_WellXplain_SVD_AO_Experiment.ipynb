{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDYf5zcZP26"
      },
      "source": [
        "# Important notes\n",
        "\n",
        " How to run the code?\n",
        "\n",
        "  - 1) To run the code first you need to upload the WellXplain.csv into the path this code is. Also you need to assign use_auth_token variable with the huggingface token to access the models. To get this access it may take few hours when you request it in your huggingface account. So you need to request it in advance before runing the code.\n",
        "  - 2) Set classifier_index variable which represents what method (model) are you going to consider.\n",
        "  - 3) you can set the ran_index with three values which provide three different random_state for data sampling.\n",
        "  - 4) run the code now in the order the cells appear.\n",
        " note that the possible values for each variable is provided in the line the the variable is assigned.\n",
        " - 5) We used A100 GPU on Google Colab for runing this code. We tried **Colab Pro**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "SEo9M7hxBazZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import XLNetModel, XLNetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_f1JIqfyBbaD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "kik6SMQyBdKS"
      },
      "outputs": [],
      "source": [
        "data_new=pd.read_csv('WellXplain.csv')\n",
        "data=pd.DataFrame()\n",
        "data['Text']=data_new['Text']\n",
        "data['Aspect']=data_new['Aspect']\n",
        "data['Aspect1']=data_new['Aspect']\n",
        "data['Aspect2']=data_new['Aspect']\n",
        "data['Aspect3']=data_new['Aspect']\n",
        "data['Aspect4']=data_new['Aspect']\n",
        "data['Explanations']=data_new['Explanations']\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i!=1:\n",
        "    data['Aspect1']=data['Aspect1'].replace(i,0)\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i==2:\n",
        "    data['Aspect2']=data['Aspect2'].replace(i,1)\n",
        "  else:\n",
        "    data['Aspect2']=data['Aspect2'].replace(i,0)\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i==3:\n",
        "    data['Aspect3']=data['Aspect3'].replace(i,1)\n",
        "  else:\n",
        "    data['Aspect3']=data['Aspect3'].replace(i,0)\n",
        "\n",
        "for i in range(1,5):\n",
        "  if i==4:\n",
        "    data['Aspect4']=data['Aspect4'].replace(i,1)\n",
        "  else:\n",
        "    data['Aspect4']=data['Aspect4'].replace(i,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "XFN_E6x9BsZs"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE= 1e-05\n",
        "\n",
        "dimension = 4\n",
        "dimensions_list = [ 'Aspect1', 'Aspect2', 'Aspect3', 'Aspect4']\n",
        "\n",
        "# threshold = 1 #\n",
        "Classifiers = [\"nghuyong/ernie-2.0-en\", \"bert-base-uncased\",\"roberta-base\" ,\"emilyalsentzer/Bio_ClinicalBERT\", \"xlnet-base-cased\",'nlptown/bert-base-multilingual-uncased-sentiment', \"mental/mental-bert-base-uncased\"]\n",
        "Classifiers_Abs = [\"ERNIE\", \"BERT\", \"RoBERTa\", \"ClinicalBERT\", \"XLNET\", \"PsychBERT\", \"Mental-BERT\"]\n",
        "\n",
        "classifier_index = 0 #set as needed [0:\"ERNIE\", 1:\"BERT\", 2:\"RoBERTa\", 3:\"ClinicalBERT\", 4:\"XLNET\", 5:\"PsychBERT\", 6:\"Mental-BERT\"]\n",
        "use_auth_token = '' # You have to assign your huggingface access token here\n",
        "\n",
        "TheClassifier = Classifiers[classifier_index]\n",
        "TheClassifier_Abstract = Classifiers_Abs[classifier_index]\n",
        "\n",
        "rand_states = [345, 200, 546]\n",
        "ran_index = 2\n",
        "rand_state = rand_states[ran_index]\n",
        "\n",
        "target_List = dimensions_list\n",
        "\n",
        "import torch\n",
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Text']\n",
        "        self.targets = self.df[target_List].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # print(index,\":\",self.title[index])\n",
        "        # print(self.title[index])\n",
        "        title = self.title[index]\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "xm8uWWbtBvWi"
      },
      "outputs": [],
      "source": [
        "if classifier_index in [0,1,2,3]:\n",
        "  tokenizer=AutoTokenizer.from_pretrained(TheClassifier)\n",
        "  class BERTClass(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(BERTClass, self).__init__()\n",
        "          self.bert_model = model = AutoModel.from_pretrained(TheClassifier, output_hidden_states=True, output_attentions=True, return_dict=True)# BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "          self.dropout = torch.nn.Dropout(0.3)\n",
        "          self.linear = torch.nn.Linear(768, dimension)\n",
        "\n",
        "      def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "          output = self.bert_model(\n",
        "              input_ids,\n",
        "              attention_mask=attn_mask,\n",
        "              token_type_ids=token_type_ids\n",
        "          )\n",
        "          output_with_attention = output\n",
        "          output_dropout = self.dropout(output.pooler_output)\n",
        "          output = self.linear(output_dropout)\n",
        "          return output, output_with_attention\n",
        "\n",
        "  model = BERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "if classifier_index==4:\n",
        "  tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "  class XLNETClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XLNETClass, self).__init__()\n",
        "        self.xlnet_model = XLNetModel.from_pretrained(\"xlnet-base-cased\", output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output_dict = self.xlnet_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        last_hidden_state = output_dict.last_hidden_state\n",
        "        attention_weights = output_dict.attentions\n",
        "        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, attention_weights[-1]\n",
        "\n",
        "  model = XLNETClass()\n",
        "  model.to(device)\n",
        "elif classifier_index==5:\n",
        "  tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  class PsychBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PsychBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "        self.model = AutoModel.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768,dimension)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = PsychBERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "elif classifier_index==6:\n",
        "\n",
        "  # !huggingface-cli login\n",
        "  from huggingface_hub import login\n",
        "\n",
        "  login(token=use_auth_token)\n",
        "  tokenizer=AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased', use_auth_token=use_auth_token) # use_auth_token has to be assign to huggingface key to access the mental bert model\n",
        "  class MentalBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MentalBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased')\n",
        "        self.model = AutoModel.from_pretrained('mental/mental-bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = MentalBERTClass()\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ROjXsBPIBzWy"
      },
      "outputs": [],
      "source": [
        "test_size = 0.2\n",
        "val_df = data.sample(frac=test_size, random_state=rand_state).reset_index (drop=True)\n",
        "train_df = data.drop (val_df.index).reset_index (drop=True)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset=CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset=CustomDataset(val_df, tokenizer,MAX_LEN)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader (\n",
        "train_dataset,\n",
        "shuffle=True,\n",
        "batch_size=TRAIN_BATCH_SIZE,\n",
        "num_workers=0\n",
        ")\n",
        "val_data_loader = torch.utils.data.DataLoader (\n",
        "valid_dataset,\n",
        "shuffle=True,\n",
        "batch_size=VALID_BATCH_SIZE,\n",
        "num_workers=0\n",
        ")\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into\n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    print(\"checkpoint_path:\",checkpoint_path)\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "6fZfNpkxB2lb"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model,\n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "\n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        # print(data['input_ids'])\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        # print(ids)\n",
        "        # raise KeyboardInterrupt\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs, _ = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # print(outputs)\n",
        "        # loss2 = loss_fn2(outputs, targets)\n",
        "\n",
        "        # print(\"loss gambler: \",loss)\n",
        "        # print(\"reservation: \", reservationn)\n",
        "\n",
        "        # print(\"loss2 CE: \", loss2)\n",
        "\n",
        "        # raise KeyboardInterrupt\n",
        "        # tar, outp = loss_fn(outputs, targets.type(torch.int64))\n",
        "        # return tar, outp\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "\n",
        "    # print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "\n",
        "    # print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs, _ = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss= loss_fn(outputs, targets)#.type(torch.int64)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      # print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics\n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch,\n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        # save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    # print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mhngWfQB7AD",
        "outputId": "fb59e7db-cb0c-4c4c-c24e-87812cf75d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tAvgerage Training Loss: 0.002834 \tAverage Validation Loss: 0.010760\n",
            "Validation loss decreased (inf --> 0.010760).  Saving model ...\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.001880 \tAverage Validation Loss: 0.008310\n",
            "Validation loss decreased (0.010760 --> 0.008310).  Saving model ...\n",
            "Epoch: 3 \tAvgerage Training Loss: 0.001424 \tAverage Validation Loss: 0.007566\n",
            "Validation loss decreased (0.008310 --> 0.007566).  Saving model ...\n",
            "Epoch: 4 \tAvgerage Training Loss: 0.001051 \tAverage Validation Loss: 0.006887\n",
            "Validation loss decreased (0.007566 --> 0.006887).  Saving model ...\n",
            "Epoch: 5 \tAvgerage Training Loss: 0.000842 \tAverage Validation Loss: 0.007401\n"
          ]
        }
      ],
      "source": [
        "import shutil, sys\n",
        "\n",
        "model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, \"ckpt_path/themodel3.pt\", \"thebestone3.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Y6esfmaNCrp6"
      },
      "outputs": [],
      "source": [
        "final_list=[]\n",
        "last_layer_attentions = []\n",
        "token_scores =[]\n",
        "for i in val_df['Text']:\n",
        "  example = i\n",
        "  encodings = tokenizer.encode_plus(\n",
        "      example,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "      output,output_with_attention = model(input_ids, attention_mask, token_type_ids)\n",
        "      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
        "      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n",
        "      # print(\"final\",final)\n",
        "      input_id_list = input_ids[0].tolist()\n",
        "      tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "      # raise KeyBoardInteruupt\n",
        "      #model_view(output_with_attention.attentions,tokens)\n",
        "\n",
        "      final_list.append(final_output[0])\n",
        "      if classifier_index in [0,1,2,3,5]:\n",
        "        attentions = output_with_attention.attentions[11][0][0]\n",
        "\n",
        "      elif classifier_index==4:\n",
        "        attentions = output_with_attention[0][11]\n",
        "\n",
        "      else:\n",
        "        attentions = output_with_attention.attentions[0]\n",
        "\n",
        "      for sample in attentions:\n",
        "        last_layer_attentions.append((sample[11]).detach().cpu())  # Detach and move to CPU\n",
        "\n",
        "\n",
        "      # Calculate attention scores for each token\n",
        "      attention_scores = torch.sum(attentions, dim=0)\n",
        "      # Normalize attention scores\n",
        "      normalized_scores = attention_scores / torch.sum(attention_scores)\n",
        "\n",
        "\n",
        "\n",
        "      # Associate each score with its corresponding token\n",
        "      token_score = {}\n",
        "      for j in range(len(normalized_scores)):\n",
        "          token = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))[j]\n",
        "          token_score[token] = normalized_scores[j].item()\n",
        "\n",
        "      token_scores.append(token_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHkm7SIfDadn",
        "outputId": "38cb611d-ae80-43f9-d462-604b7b4a07ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nghuyong/ernie-2.0-en\n",
            "AO score: 0.2588996763754045\n",
            "Number of samples with the ground truth explanations: 160.0 Number of total samples: 618\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "top = 4\n",
        "Correct_samples = np.zeros((len(token_scores)))\n",
        "for i in range(len(token_scores)):\n",
        "  mytoken = token_scores[i]\n",
        "  token_remove = []\n",
        "  Explanation = (val_df.iloc[i]['Explanations']).split()\n",
        "  for tok in mytoken.keys():\n",
        "    if  tok in ['[SEP]', '[CLS]']:\n",
        "      token_remove.append(tok)\n",
        "  for item in token_remove:\n",
        "    del mytoken[item]\n",
        "  token_sorted = dict(sorted(mytoken.items(), key=lambda x:x[1], reverse=True))\n",
        "\n",
        "  token_sorted_top = dict(list(token_sorted.items())[0:top])\n",
        "  # print(token_sorted_top)\n",
        "  # raise KeyboardInterrupt\n",
        "\n",
        "  common_token = 0\n",
        "  for item in Explanation:\n",
        "    if item in token_sorted_top.keys():\n",
        "      common_token += 1\n",
        "  # print(Explanation, token_sorted_top, common_token)\n",
        "  if common_token/len(Explanation)>.5:\n",
        "    Correct_samples[i] = 1\n",
        "  # raise KeyboardInterrupt\n",
        "print(TheClassifier)\n",
        "print('AO score:', sum(Correct_samples)/len(token_scores))\n",
        "print('Number of samples with the ground truth explanations:',sum(Correct_samples),'Number of total samples:', len(token_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPzha-J1B88c",
        "outputId": "3c834b2e-df29-44d6-9978-d0f96757fcaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***********************************\n",
            "Model: ERNIE\n",
            "Epochs: 5\n",
            "Batch_size: 16\n",
            "Max_len: 128\n",
            "Learning Rate: 1e-05\n",
            "Rand_state: 546\n",
            "SVD_ranking : 71\n"
          ]
        }
      ],
      "source": [
        "last_layer_attentions = []\n",
        "for batch_idx, data in enumerate(val_data_loader):\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        outputs, output_with_attention = model(ids, mask, token_type_ids)\n",
        "\n",
        "        if classifier_index==4:\n",
        "           attentions = output_with_attention # For XLNET\n",
        "        else:\n",
        "           attentions = output_with_attention.attentions[0]\n",
        "\n",
        "        for sample in attentions:\n",
        "          last_layer_attentions.append((sample[11]).cpu())\n",
        "        # Clear GPU memory\n",
        "        del ids, mask, token_type_ids, outputs, output_with_attention\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "from numpy.linalg import svd\n",
        "from numpy.linalg import matrix_rank\n",
        "\n",
        "d=[item.detach().numpy() for item in last_layer_attentions]\n",
        "U, S, VT = svd(d)\n",
        "print('***********************************')\n",
        "print(\"Model:\", TheClassifier_Abstract)\n",
        "print(\"Epochs:\", EPOCHS)\n",
        "print(\"Batch_size:\",TRAIN_BATCH_SIZE)\n",
        "print(\"Max_len:\", MAX_LEN)\n",
        "print(\"Learning Rate:\", LEARNING_RATE)\n",
        "print(\"Rand_state:\", rand_state)\n",
        "print(\"SVD_ranking :\", matrix_rank(S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "BD2q0P02B_CD"
      },
      "outputs": [],
      "source": [
        "val_list=[]\n",
        "for i in range(len(val_df)):\n",
        "  val_list.append(val_df[target_List][i:i+1].values.tolist()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "i0cizppACbyE"
      },
      "outputs": [],
      "source": [
        "def finalLabels(predicted_list,val_list):\n",
        "  for i in range(len(predicted_list)):\n",
        "    indices=np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))]\n",
        "    # argsort()[:-1][:n]\n",
        "    # print(predicted_list,np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))])\n",
        "    for j in range(len(predicted_list[i])):\n",
        "      if j in indices:\n",
        "        predicted_list[i][j]=1.0\n",
        "      else:\n",
        "        predicted_list[i][j]=0.0\n",
        "  return predicted_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py4WhRNSCd7E",
        "outputId": "b5405688-7233-48ef-b646-ad413004e8ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 1.0],\n",
              " [0.0, 0.0, 1.0, 0.0]]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finalLabels(final_list,val_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HKDE158CfaB",
        "outputId": "87da8d86-191b-46fe-ecdd-d2f5abd75f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Aspect1       0.95      0.38      0.54       165\n",
            "     Aspect2       0.74      0.98      0.84       106\n",
            "     Aspect3       0.89      0.98      0.94       242\n",
            "     Aspect4       0.65      0.90      0.76       105\n",
            "\n",
            "   micro avg       0.81      0.81      0.81       618\n",
            "   macro avg       0.81      0.81      0.77       618\n",
            "weighted avg       0.84      0.81      0.78       618\n",
            " samples avg       0.81      0.81      0.81       618\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "label_names = target_List\n",
        "print(classification_report(val_list, final_list,target_names=label_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOfsOzTpHWw1",
        "outputId": "60200129-642b-4880-d2ad-9a937ca7868e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons==0.16.1 in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.16.1) (4.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.16.1) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons==0.16.1\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GGMuUSkC8XS",
        "outputId": "e809a9f2-1d99-438f-e6e6-9e3bdf27e2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7525\n",
            "target_List: ['Aspect1', 'Aspect2', 'Aspect3', 'Aspect4']\n",
            "0.5323008064209725\n",
            "0.816430535741492\n",
            "0.8960964706232821\n",
            "0.7120144186016257\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "metric = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=dimension)\n",
        "metric.update_state(val_list,final_list)\n",
        "result = metric.result()\n",
        "print(result.numpy())\n",
        "print('target_List:', target_List)\n",
        "\n",
        "val = pd.DataFrame(val_list, columns = target_List)\n",
        "fin = pd.DataFrame(final_list, columns = target_List)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(matthews_corrcoef(val[\"Aspect1\"],fin[\"Aspect1\"]))\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(matthews_corrcoef(val[\"Aspect2\"],fin[\"Aspect2\"]))\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(matthews_corrcoef(val[\"Aspect3\"],fin[\"Aspect3\"]))\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(matthews_corrcoef(val[\"Aspect4\"],fin[\"Aspect4\"]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
