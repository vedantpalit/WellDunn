{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This code is for Gambler loss scenario on MultiWD dataset.\n",
        " How to run the code?\n",
        "\n",
        "  - 1) To run the code first you need to upload the MultiWD.csv into the path this code is.\n",
        "  - 2) Set classifier_index variable which represents what method (model) are you going to consider.\n",
        "  - 3) Set target_index varibale which represents what dimension you want to run the code on. It could be 0 for 6-dimension, 1 for 5-dimension, and 2 for 4-dimension.\n",
        "  - 4) you can set the ran_index with three values which provide three different random_state for data sampling.\n",
        "  - 5) run the code now.\n",
        " note that the possible values for each variable is provided in the line the the variable is assigned."
      ],
      "metadata": {
        "id": "pasr2uVd9bxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension=6\n",
        "# We tried three different random_state for sampling data: 200, 345, and 546.\n",
        "# It sets 200 as a defualt. You can change it in this cell if you want.\n",
        "rand_state=200\n",
        "\n",
        "classifier_index = 1  #Models [0:\"ERNIE\", 1:\"BERT\", 2:\"RoBERTa\", 3:\"ClinicalBERT\", 4:\"XLNET\", 5:\"PsychBERT\", 6:\"Mental-BERT\"]\n",
        "# Do not change the following.\n",
        "MAX_LEN = 64\n",
        "TRAIN_BATCH_SIZE = 32 # for models except XLNET and MentalBERT it is 32, else it is 2 (due to memory issue)\n",
        "VALID_BATCH_SIZE = 32 # for models except XLNET and MentalBERT it is 32, else it is 2 (due to memory issue)\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE= 1e-05"
      ],
      "metadata": {
        "id": "Cyt5pJWK9mLO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Classifiers = [\"nghuyong/ernie-2.0-en\", \"bert-base-uncased\",\"roberta-base\" ,\"emilyalsentzer/Bio_ClinicalBERT\", \"xlnet-base-cased\",'nlptown/bert-base-multilingual-uncased-sentiment', \"mental/mental-bert-base-uncased\"]\n",
        "Classifiers_Abs = [\"ERNIE\", \"BERT\", \"RoBERTa\", \"ClinicalBERT\", \"XLNET\", \"PsychBERT\", \"Mental-BERT\"]\n",
        "TheClassifier = Classifiers[classifier_index]\n",
        "TheClassifier_Abstract = Classifiers_Abs[classifier_index]"
      ],
      "metadata": {
        "id": "iytkCmJe_dJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_settings =[\n",
        "   ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational','Emotional'],\n",
        "   ['Physical', 'Intellectual', 'Social', 'Vocational','Spiritual_Emotional'],\n",
        "   [ 'Intellectual', 'Social', 'Vocational','Physical_Spiritual_Emotional'],\n",
        "   [ 'Social', 'Intellectual_Vocational','Physical_Spiritual_Emotional']\n",
        "]\n",
        "if dimension==6:\n",
        "  target_index=0\n",
        "elif dimension==5:\n",
        "  target_index=1 # [0: 6-dim, 1:5-dim, 2:4-dim, 3:3-dim] #Set dimension value\n",
        "elif dimension==4:\n",
        "  target_index=2\n",
        "else:\n",
        "  target_index=3\n",
        "target_List = targets_settings[target_index]\n",
        "\n",
        "\n",
        "print('dimension:',dimension)\n",
        "print('target_list:',target_List)\n",
        "print('Model:',TheClassifier_Abstract)\n",
        "print('Random State:',rand_state)"
      ],
      "metadata": {
        "id": "_-fJ95SiLIQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs and Utils"
      ],
      "metadata": {
        "id": "Ogo6AbQh9Meb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, requirements like numpy, pandas libraries are imported. Also, the pretrained models that we are using in this code are downloaded in this section.\n",
        "\n",
        "Action required: you may need to log in into hugginface for usage of MentalBERT"
      ],
      "metadata": {
        "id": "-0zFWUU59q4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow_addons\n",
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "cFdkfHWWMjTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Prep"
      ],
      "metadata": {
        "id": "giF7jIZwLzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, XLNetTokenizer,XLNetModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(TheClassifier)\n",
        "import torch\n",
        "data=pd.read_csv('MultiWD.csv')"
      ],
      "metadata": {
        "id": "YPl3wn3IL5vB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data if we have selected 6-dimension\n",
        "if dimension ==6:\n",
        "  data=data.astype({'Spiritual':'float', 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float', 'Emotional':'float'})\n",
        "  target_List = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational','Emotional']\n",
        "# Preparing the data if we have selected 5-dimension\n",
        "elif dimension == 5:\n",
        "  data=data.astype({'Spiritual':'int32', 'Physical':'int32', 'Intellectual':'int32', 'Social':'int32', 'Vocational':'int32', 'Emotional':'int32'})\n",
        "  data ['Spiritual or Emotional'] = data.Spiritual | data.Emotional\n",
        "  data = data.drop(['Spiritual', 'Emotional'], axis=1)\n",
        "  data=data.astype({ 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float',\n",
        "        'Spiritual or Emotional':'float'})\n",
        "  target_List = ['Physical', 'Intellectual', 'Social', 'Vocational','Spiritual or Emotional']\n",
        "# Preparing the data if we have selected 4-dimension\n",
        "elif dimension == 4:\n",
        "  data=data.astype({'Spiritual':'int32', 'Physical':'int32', 'Intellectual':'int32', 'Social':'int32', 'Vocational':'int32', 'Emotional':'int32'})\n",
        "  data ['Spiritual or Emotional'] = data.Spiritual | data.Emotional\n",
        "  data = data.drop(['Spiritual', 'Emotional'], axis=1)\n",
        "  data=data.astype({ 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float',\n",
        "        'Spiritual or Emotional':'float'})\n",
        "\n",
        "  target_List = ['Physical', 'Intellectual', 'Social', 'Vocational','Spiritual or Emotional']\n",
        "  data=data.astype({ 'Physical':'int32', 'Intellectual':'int32', 'Social':'int32', 'Vocational':'int32','Spiritual or Emotional':'int32'})\n",
        "  data ['Spiritual or Emotional or Physical'] = data ['Spiritual or Emotional'] | data['Physical']\n",
        "  target_List = ['Physical', 'Intellectual', 'Social', 'Vocational','Spiritual or Emotional']\n",
        "  # data=data.astype({ 'Physical':'float', 'Intellectual':'float', 'Social':'float', 'Vocational':'float','Spiritual or Emotional':'float'})\n",
        "\n",
        "  data=data.astype({'Spiritual or Emotional':'int32', 'Physical':'int32', 'Intellectual':'int32', 'Social':'int32', 'Vocational':'int32', })\n",
        "\n",
        "  data = data.drop(['Physical'], axis=1)\n",
        "  data=data.drop(['Spiritual or Emotional'], axis=1)\n",
        "  data=data.astype({  'Intellectual':'float', 'Social':'float', 'Vocational':'float','Spiritual or Emotional or Physical':'float'})\n",
        "  target_List = ['Intellectual', 'Social', 'Vocational','Spiritual or Emotional or Physical']\n",
        "\n",
        "else:\n",
        "  print(\"dimension can be 6, 5, or 4. Please make sure you select one of these\")\n"
      ],
      "metadata": {
        "id": "q8Fa-vR_L1k3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):   #Dataset Preparation\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['text']\n",
        "        self.targets = self.df[target_List].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }\n",
        "\n",
        "test_size = 0.2\n",
        "val_df = data.sample(frac=test_size, random_state=rand_state).reset_index (drop=True) #Formation of train and test sets\n",
        "train_df = data.drop (val_df.index).reset_index (drop=True)\n",
        "train_dataset=CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset=CustomDataset(val_df,tokenizer,MAX_LEN)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader (\n",
        "train_dataset,\n",
        "shuffle=True,\n",
        "batch_size=TRAIN_BATCH_SIZE,\n",
        "num_workers=0\n",
        ")\n",
        "val_data_loader = torch.utils.data.DataLoader (\n",
        "valid_dataset,\n",
        "shuffle=False,\n",
        "batch_size=VALID_BATCH_SIZE,\n",
        "num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "bEk8hjKgMR4M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection and Model Save/Load"
      ],
      "metadata": {
        "id": "41tu8YIFMaFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #switches device to cpu if gpu is unavailable\n",
        "print(device)\n",
        "\n",
        "# Functions for saving and loading the model in the case the training\n",
        "# is interrupted. In this case, we use these functions start training\n",
        "# again from last check point.\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into\n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)\n"
      ],
      "metadata": {
        "id": "Nuk67QrEMfS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if classifier_index==0:\n",
        "  tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-2.0-en')\n",
        "  class ERNIEClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ERNIEClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-2.0-en')\n",
        "        self.ernie_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-en', output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output_dict = self.ernie_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        last_hidden_state = output_dict.last_hidden_state\n",
        "        attention_weights = output_dict.attentions\n",
        "        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, attention_weights[-1]\n",
        "\n",
        "  model = ERNIEClass()\n",
        "  model.to(device)\n",
        "\n",
        "if classifier_index==1:\n",
        "  tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "  class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = AutoModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "\n",
        "\n",
        "  model = BERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "if classifier_index==2:\n",
        "  tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "  class roBERTaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(roBERTaClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "        self.model = AutoModel.from_pretrained('roberta-base',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model = roBERTaClass()\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "if classifier_index==3:\n",
        "  tokenizer=AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "  class ClinicalBIGBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClinicalBIGBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "        self.model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "  model = ClinicalBIGBERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "if classifier_index==4:\n",
        "  class XLNETClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XLNETClass, self).__init__()\n",
        "        self.xlnet_model = XLNetModel.from_pretrained(\"xlnet-base-cased\", output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension + 1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output_dict = self.xlnet_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        last_hidden_state = output_dict.last_hidden_state\n",
        "        attention_weights = output_dict.attentions\n",
        "        output_dropout = self.dropout(last_hidden_state[:, -1, :])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, attention_weights[-1]\n",
        "\n",
        "  model = XLNETClass()\n",
        "  model.to(device)\n",
        "\n",
        "elif classifier_index==5:\n",
        "  tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  class PsychBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PsychBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "        self.model = AutoModel.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768,dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = PsychBERTClass()\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "elif classifier_index==6:\n",
        "  !huggingface-cli login\n",
        "  tokenizer=AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased',use_auth_token=True)\n",
        "  class MentalBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MentalBERTClass, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('mental/mental-bert-base-uncased')\n",
        "        self.model = AutoModel.from_pretrained('mental/mental-bert-base-uncased',output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, dimension+1)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, seg_ids):\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=seg_ids\n",
        "        )\n",
        "        output_with_attention = output\n",
        "        output_dropout = self.dropout(output.last_hidden_state[:, 0])\n",
        "        output = self.linear(output_dropout)\n",
        "        return output, output_with_attention\n",
        "\n",
        "  model = MentalBERTClass()\n",
        "  model.to(device)"
      ],
      "metadata": {
        "id": "DxB4_W0dMePq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function - Gamblers"
      ],
      "metadata": {
        "id": "Mb2_vNDiM5Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(m_outputs, targets):\n",
        "        reward = dimension\n",
        "\n",
        "        tensor_temp = torch.zeros(32,dtype=torch.float)\n",
        "        tensor_temp.to(device)\n",
        "        outputs = torch.nn.functional.softmax(m_outputs, dim=1,dtype=torch.float)\n",
        "\n",
        "        outputs, reservation = outputs[:, :-1], outputs[:, -1]\n",
        "\n",
        "        # gain = torch.gather(outputs, dim=1, index=targets).squeeze()\n",
        "        # print(\"targets:\",targets)\n",
        "        # print(\"outputs:\", outputs)\n",
        "        # raise KeyboardInterrupt\n",
        "        # return targets, outputs\n",
        "        gain = torch.einsum(\"ij, ij -> i\", targets.to(torch.float), outputs)\n",
        "\n",
        "        # doubling_rate = (gain.max() + reservation / reward).log()\n",
        "        doubling_rate = -torch.log(gain + reservation/reward)\n",
        "        return  doubling_rate.mean(), reservation"
      ],
      "metadata": {
        "id": "HZKEg2noM7yx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "kVzjkVxYLAEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js_QGa1jK9xu"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model,\n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "\n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        # print(data['input_ids'])\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        # print(ids)\n",
        "        # raise KeyboardInterrupt\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs, _ = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, reservationn = loss_fn(outputs, targets.type(torch.int64))\n",
        "\n",
        "        # print(outputs)\n",
        "        # loss2 = loss_fn2(outputs, targets)\n",
        "\n",
        "        # print(\"loss gambler: \",loss)\n",
        "        # print(\"reservation: \", reservationn)\n",
        "\n",
        "        # print(\"loss2 CE: \", loss2)\n",
        "\n",
        "        # raise KeyboardInterrupt\n",
        "        # tar, outp = loss_fn(outputs, targets.type(torch.int64))\n",
        "        # return tar, outp\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "\n",
        "    # print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "\n",
        "    # print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs, _ = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss, _ = loss_fn(outputs, targets.type(torch.int64))\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      # print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics\n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch,\n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        # save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    # print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, sys\n",
        "\n",
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, \"ckpt_path/themodel3.pt\", \"thebestone3.pt\")"
      ],
      "metadata": {
        "id": "BmWoHIqnDKXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculation of SVD Ranking"
      ],
      "metadata": {
        "id": "vS5hhhSqDRNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This piece of code is meant to calculate the singular value decomposition rank for each of the models"
      ],
      "metadata": {
        "id": "sxmROORMH7Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer_attentions = []\n",
        "for batch_idx, data in enumerate(train_data_loader):\n",
        "    ids = data['input_ids'].to(device, dtype=torch.long)\n",
        "    mask = data['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "    outputs, output_with_attention = model(ids, mask, token_type_ids)\n",
        "\n",
        "    if classifier_index == 4:\n",
        "        attentions = output_with_attention  # For XLNET\n",
        "    else:\n",
        "        attentions = output_with_attention.attentions[0]\n",
        "\n",
        "    for sample in attentions:\n",
        "        last_layer_attentions.append((sample[11]).detach().cpu())  # Detach and move to CPU\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del ids, mask, token_type_ids, outputs, output_with_attention\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pyGG3M0rDKp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import svd\n",
        "from numpy.linalg import matrix_rank\n",
        "\n",
        "d=[item.detach().numpy() for item in last_layer_attentions]\n",
        "U, S, VT = svd(d)\n",
        "\n",
        "print('*********************************************')\n",
        "# print(\"Experiment:\", dimension)\n",
        "print(\"SVD_ranking:\", matrix_rank(S))"
      ],
      "metadata": {
        "id": "xWyaFB6ID6ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics"
      ],
      "metadata": {
        "id": "5wPhY7H3NC5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finalLabels2(predicted_list,val_list):\n",
        "\n",
        "  indices=np.array(predicted_list).argsort()[::-1][:int(sum(val_list))]\n",
        "  for j in range(len(predicted_list)):\n",
        "    if j in indices:\n",
        "      predicted_list[j]=1.0\n",
        "    else:\n",
        "      predicted_list[j]=0.0\n",
        "  return predicted_list\n",
        "\n",
        "\n",
        "def finalLabels(predicted_list,val_list):\n",
        "  for i in range(len(predicted_list)):\n",
        "    indices=np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))]\n",
        "    # argsort()[:-1][:n]\n",
        "    # print(predicted_list,np.array(predicted_list[i]).argsort()[::-1][:int(sum(val_list[i]))])\n",
        "    for j in range(len(predicted_list[i])):\n",
        "      if j in indices:\n",
        "        predicted_list[i][j]=1.0\n",
        "      else:\n",
        "        predicted_list[i][j]=0.0\n",
        "  return predicted_list\n",
        "\n",
        "\n",
        "final_list= []\n",
        "examples = []\n",
        "# for i in val_df['text']:\n",
        "#   example = i\n",
        "Final_Outputs = []\n",
        "for i in range(len(val_df)):\n",
        "  example  = val_df.loc[i]['text']\n",
        "  target  = (val_df.loc[i][target_List]).tolist()\n",
        "  encodings = tokenizer.encode_plus(\n",
        "      example,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "      output, _ = model(input_ids, attention_mask, token_type_ids)\n",
        "      temp = torch.Tensor(target).type(torch.int64).to(device)\n",
        "      loss, reservation = loss_fn(output,temp.reshape([1,dimension]))\n",
        "      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "      # print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
        "      # final=[0 if i<0.4 else 1 for i in final_output[0]]\n",
        "      # print(\"final\",final)\n",
        "      # print(final_output[0][:-1], target)\n",
        "      temp = finalLabels2(final_output[0][:-1],target)\n",
        "      # print(temp)\n",
        "\n",
        "      # raise KeyInterruption\n",
        "      # print(\"tepm: \",temp)\n",
        "      # print(\"target:\", target.tolist())\n",
        "      # print(\"All: \", temp+target+torch.Tensor.tolist(reservation))\n",
        "\n",
        "      final_list.append(temp+target+torch.Tensor.tolist(reservation))\n",
        "      examples.append(example)\n",
        "      Final_Outputs.append({'text':example,'target':target, 'output':temp, 'reservation':reservation})\n",
        "      # record\n",
        "\n",
        "from operator import itemgetter\n",
        "sorted_final_list = sorted(final_list, key=itemgetter(2*dimension - 1), reverse = False)\n",
        "\n",
        "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
        "\n",
        "def get_accuracies(true_labels, predictions):\n",
        "    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
        "    cm = multilabel_confusion_matrix(true_labels, predictions)\n",
        "    total_count = np.array(true_labels).shape[0]\n",
        "    accuracies = []\n",
        "    # print(np.array(true_labels).shape[1])\n",
        "    # raise KeyboardInterrupt\n",
        "    for i in range(np.array(true_labels).shape[1]):\n",
        "        true_positive_count = np.sum(cm[i,1,1]).item()\n",
        "        true_negative_count = np.sum(cm[i,0,0]).item()\n",
        "        accuracy = (true_positive_count + true_negative_count) / total_count\n",
        "        accuracies.append(accuracy)\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "label_names = target_List\n",
        "for threshold in [1, .95, 0.9, 0.85, 0.8,.75]:\n",
        "  list_data = sorted_final_list[:round(threshold*len(sorted_final_list)-1)]\n",
        "  val_list = [list_data[i][dimension:2*dimension] for i in range(len(list_data))]\n",
        "  prediction = [list_data[i][0:dimension] for i in range(len(list_data))]\n",
        "  # print(val_list)\n",
        "  # print(prediction)\n",
        "  # print(dimension,len(val_list), len(prediction))\n",
        "  # raise KeyboardInterrupt\n",
        "  print('############# '+TheClassifier_Abstract+'_Dim'+str(dimension)+'_run'+str(rand_state)+'_threshold'+str(threshold)+'   #############')\n",
        "\n",
        "  print(classification_report(val_list, prediction,target_names=label_names))\n",
        "\n",
        "  accuracies = get_accuracies(val_list,prediction)\n",
        "  accuracies = [round(accuracies[i],2) for i in range(dimension)]\n",
        "  print(\"accuracies for each class:\",accuracies)\n",
        "\n",
        "  val = pd.DataFrame(val_list, columns = target_List)\n",
        "  fin = pd.DataFrame(prediction, columns = target_List)\n",
        "\n",
        "  print('MCC:')\n",
        "\n",
        "  for i in range(dimension):\n",
        "     label = target_List[i]\n",
        "     print(label, matthews_corrcoef(val[label],fin[label]))\n",
        "#   print(\"Physical\", matthews_corrcoef(val[\"Physical\"],fin[\"Physical\"]))\n",
        "#   print(\"Spiritual\", matthews_corrcoef(val[\"Spiritual\"],fin[\"Spiritual\"]))\n",
        "#   print(\"Intellectual\", matthews_corrcoef(val[\"Intellectual\"],fin[\"Intellectual\"]))\n",
        "#   print(\"Social\", matthews_corrcoef(val[\"Social\"],fin[\"Social\"]))\n",
        "#   print(\"Vocational\", matthews_corrcoef(val[\"Vocational\"],fin[\"Vocational\"]))\n",
        "#   print(\"Emotional\", matthews_corrcoef(val[\"Emotional\"],fin[\"Emotional\"]))\n",
        "\n",
        "print('Done!!!!!!!!!!!!')"
      ],
      "metadata": {
        "id": "aI1lSn0LNERf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}